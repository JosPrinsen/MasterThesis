---
title: "MasterThesisRPart"
output: html_document
date: "2024-05-01"
---


```{r}

#
#                  stats                graphics               grDevices                   utils                datasets                 methods 
#           "stats base"         "graphics base"        "grDevices base"            "utils base"         "datasets base"          "methods base" 
#                   base               gridExtra                  ggpubr                    coin                 rstatix                     FSA 
#            "base base"         "gridExtra 2.3"          "ggpubr 0.6.0"            "coin 1.4-3"         "rstatix 0.7.2"             "FSA 0.9.5" 
#           multcompView                multcomp                 TH.data                    MASS                survival                 mvtnorm 
#  "multcompView 0.1-10"       "multcomp 1.4-25"         "TH.data 1.1-2"         "MASS 7.3-58.4"        "survival 3.5-5"         "mvtnorm 1.2-4" 
#                emmeans                lmerTest                   tidyr                    lme4                  Matrix                 ggplot2 
#       "emmeans 1.10.1"        "lmerTest 3.1-3"           "tidyr 1.3.0"         "lme4 1.1-35.1"          "Matrix 1.6-5"         "ggplot2 3.5.1" 
#                  dplyr       fractalRegression 
#          "dplyr 1.1.2" "fractalRegression 1.1" 




# Get information about all loaded packages
loaded_packages <- sessionInfo()

# Create a list to store package names and versions for base packages
base_package_info <- sapply(loaded_packages$basePkgs, function(pkg) paste(pkg, "base", sep = " "))

# Create a list to store package names and versions for other packages
other_package_info <- sapply(loaded_packages$otherPkgs, function(pkg) paste(pkg$Package, pkg$Version, sep = " "))

# Combine base and other packages info
all_package_info <- c(base_package_info, other_package_info)

# Print the package names along with their versions
print(all_package_info)


``` 



#####
#This is where R begins.
#####

```{r}
#Set packages and import data from python
set.seed(34534534)
if (!requireNamespace("fractalRegression", quietly = TRUE)) install.packages("fractalRegression")
require(fractalRegression) # (Likens A, Wiltshire T 2023)
# Install necessary packages if you haven't already
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
if (!requireNamespace("lme4", quietly = TRUE))install.packages("lme4")
library(dplyr)
library(ggplot2)

library(lme4)


library(dplyr)
library(tidyr)  # This is crucial for pivot_longer()
library(ggplot2)
library(lme4)   # Assuming you're still working with mixed-effects models
library(lmerTest)
library(emmeans)


#load("MThesisData/ThesisDataframe.RData")
#load("MThesisData/Raw_data.RData")
#load("MThesisData/MFDFA_trials_list.RData")
#load("MThesisData/MFDFA_width_trials_list.RData")

#save(Raw_data, file="E:\\MonsterDataset\\R_Raw_Data.Rdata")
#save(ERD_data_New2, file="E:\\MonsterDataset\\R_ERD_data.Rdata")



# Load the CSV file into a data frame
ERD_data <- read.csv("E:\\MonsterDataset\\PreprocessedData\\ERD_data.csv")


slopes_data_C3 <- read.csv("E:\\MonsterDataset\\PreprocessedData\\slopes_data_C3.csv")
slopes_data_C4 <- read.csv("E:\\MonsterDataset\\PreprocessedData\\slopes_data_C4.csv") #Still have to add this
slopes_data_C3 <- slopes_data_C3 %>% mutate(index = row_number())
slopes_data_C4 <- slopes_data_C4 %>% mutate(index = row_number())





# Combine the dataframes by interleaving rows
combined_slopes_C3_C4 <- bind_rows(slopes_data_C3, slopes_data_C4) %>%
  arrange(index, .by_group = TRUE) %>%
  select(-index)  # Optionally remove the index column if no longer needed

ERD_data["combined_slopes_C3_C4"] <- combined_slopes_C3_C4


mfdfa_rest_data <- read.csv("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mf_dfa_widths_rest.csv")
ERD_data["mf_dfa_rest_widths"] <- mfdfa_rest_data


mf_dfa_widths <- read.csv("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mf_dfa_widths.csv") 
ERD_data["mf_dfa_widths"] <- mf_dfa_widths

ERD_data <- ERD_data %>%
  group_by(Subject, Session) %>%
  mutate(
    Trials = (row_number() + 1) %/% 2  # row_number() starts at 1; adding 1 ensures that the division results in 1, 1, 2, 2, etc.
  )


ERD_data_New2 <- subset(ERD_data, artifact == 0)

# na_indices_ERD = which(is.na(Longitudinal$dfa_ERD_avg))
# 
# # Find indices where dfa_mfdfa_avg is NA
# na_indices_MFDFA = which(is.na(Longitudinal$dfa_mfdfa_avg))
# na_indices_MFDFA2 <- na_indices_MFDFA*2
# 
# na_indices_MFDFA3 <- numeric(0)
# # Loop through each item in the original vector
# for (item in na_indices_MFDFA2) {
#   na_indices_MFDFA3 <- c(na_indices_MFDFA3, item - 1, item)
# }
# 
# # Print the new vector
# print(na_indices_MFDFA3)
# 
# ERD_data_New2 <- ERD_data_New2[-na_indices_MFDFA3, ]

```


#Lateralization Index
```{r}
power_means <- ERD_data_New2 %>%
  group_by(Subject, Session, Channel, targetnumber) %>%
  summarise(mean_power = mean(ERD, na.rm = TRUE), .groups = 'drop')

# Step 2: Spread the data for easier calculation (reshape the data)
library(tidyr)
power_wide <- power_means %>%
  pivot_wider(names_from = c(Channel, targetnumber), values_from = mean_power,
              names_sep = "")  # This creates columns like C31, C32, C41, C42

# Step 3: Calculate the LI for each Subject and Session
# LI = (C32 - C42) / (C31 - C41)
power_wide <- power_wide %>%
  #mutate(LI = (C32 - C42) / (C31 - C41))
  mutate(LI = ((C32 - C42) + (C41 - C31)) /2)

# View results
print(power_wide)



summed_mfdfa_dif <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(mfdfa_dif_values = (mean(mf_dfa_widths)- mean(mf_dfa_rest_widths)))# 


mfdfa_means <- ERD_data_New2 %>%
  group_by(Subject, Session, Channel, targetnumber) %>%
  summarise(mean_mfdfa_widths = mean(mf_dfa_widths, na.rm = TRUE), .groups = 'drop')

mfdfa_dif_means <- ERD_data_New2 %>%
  group_by(Subject, Session, Channel, targetnumber) %>%
  summarise(mean_mfdfa_widths = (mean(mf_dfa_widths)), .groups = 'drop')


# Assuming mfdfa_dif_means is your data frame
mfdfa_dif_means$targetnumber <- ifelse(mfdfa_dif_means$targetnumber == 1, "right", 
                                       ifelse(mfdfa_dif_means$targetnumber == 2, "left", mfdfa_dif_means$targetnumber))

# Step 2: Spread the data for easier calculation (reshape the data)
library(tidyr)
mfdfa_wide <- mfdfa_dif_means %>%
  pivot_wider(names_from = c(Channel, targetnumber), values_from = mean_mfdfa_widths,
              names_sep = "")  # This creates columns like C31, C32, C41, C42

# Step 3: Calculate the LI for each Subject and Session
# LI = (C32 - C42) / (C31 - C41)
mfdfa_wide <- mfdfa_wide %>%
  #mutate(LI_mfdfa = (C32 - C42) / (C31 - C41))
  mutate(LI_mfdfa = ((C3left - C4left) + (C4right - C3right)) /2)

# View results
print(mfdfa_wide)




```

```{r}


performance <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(accuracy = mean(forcedresult))

summed_mfdfa <- ERD_data_New2 %>%
  group_by(Subject,Session) %>%
  summarise(mfdfavalues = mean(mf_dfa_widths))# 

summed_mfdfa_dif <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(mfdfa_dif_values = (mean(mf_dfa_widths)- mean(mf_dfa_rest_widths)))# 

summed_ERD <- ERD_data_New2 %>%
  group_by(Subject,Session) %>%
  summarise(ERDvalues = mean(ERD))# 

performance_mfdfa <- merge(summed_mfdfa, performance)
performance_mfdfa <- merge(performance_mfdfa, summed_mfdfa_dif)
performance_mfdfa <- merge(performance_mfdfa, summed_ERD)

performance <- merge(performance, power_wide)
performance["mfdfa_dif_values"] <- performance_mfdfa$mfdfa_dif_values

performance_mfdfa <- merge(performance_mfdfa, mfdfa_wide)



kmeans_result <- kmeans(performance$accuracy, centers = 2)

# Add cluster results to the dataframe
performance$cluster <- kmeans_result$cluster

# To ensure consistent labeling, you might want to relabel the clusters based on mean accuracy
cluster_labels <- aggregate(accuracy ~ cluster, performance, mean)
names(cluster_labels) <- c('cluster', 'mean_accuracy')
cluster_labels <- cluster_labels[order(-cluster_labels$mean_accuracy),]
performance$cluster <- factor(performance$cluster, levels = cluster_labels$cluster, labels = c('High', 'Low'))

performance_mfdfa["cluster"] <- performance$cluster
LateralizationDF <- data.frame(LImfdfa = performance_mfdfa$LI_mfdfa, LIERD = performance$LI, accuracy = performance_mfdfa$accuracy, cluster = performance$cluster)

# Reshape the dataframe to a long format suitable for comparison
LateralizationDF_long <- pivot_longer(LateralizationDF, 
                                      cols = c(LImfdfa, LIERD), 
                                      names_to = "Condition", 
                                      values_to = "Values")





library(multcomp)
library(multcompView)


library(FSA) # For Dunn's test
library(rstatix) # For Wilcoxon tests and effect sizes
library(coin)





metrics <- rep(c("LI-MFDFA", "LI-ERD"), length.out = 868)
LateralizationDF_long["Metric"] <- metrics

LateralizationDF_long <- LateralizationDF_long %>%
  mutate(PairID = rep(1:(n()/2), each = 2))


identify_outliers <- function(df) {
  Q1 <- quantile(df$Values, 0.25)
  Q3 <- quantile(df$Values, 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Identify outliers in the data
  df$Outlier <- df$Values < lower_bound | df$Values > upper_bound
  
  # If either in a pair is an outlier, mark both
  df$Outlier <- ave(df$Outlier, df$PairID, FUN = any)
  
  return(df)
}

# Apply function to dataset
LateralizationDF_long <- LateralizationDF_long %>%
  group_by(cluster) %>%
  do(identify_outliers(.))

# Remove outliers
LateralizationDF_long_noOut <- LateralizationDF_long %>%
  filter(!Outlier)





descriptive_stats <- LateralizationDF_long_noOut %>%
  group_by(Metric, cluster) %>%
  summarise(
    count = n(),
    mean = mean(Values, na.rm = TRUE),
    sd = sd(Values, na.rm = TRUE)
  )
print(descriptive_stats)

shapiro_test_results <- LateralizationDF_long_noOut %>%
  group_by(Metric, cluster) %>%
  summarise(p_value = shapiro.test(Values)$p.value)
print(shapiro_test_results)

kruskal_test_results <- LateralizationDF_long_noOut %>%
  group_by(Metric) %>%
  rstatix::kruskal_test(Values ~ cluster)
print(kruskal_test_results)








pairwise_comparisons <- function(df) {
  pairwise_results <- pairwise.wilcox.test(df$Values, df$cluster, p.adjust.method = "bonferroni")
  return(pairwise_results$p.value)
}

pairwise_results_no_outliers <- LateralizationDF_long_noOut %>%
  group_by(Metric) %>%
  do(tibble(pairwise_p_values = list(pairwise_comparisons(.))))
print(pairwise_results_no_outliers)

# Combine results for annotations
annotations <- pairwise_results_no_outliers %>%
  unnest(pairwise_p_values) %>%
  mutate(y.position = c(10))  # Adjust these positions based on your data range


# Effect size calculation
effect_size_results_no_outliers <- LateralizationDF_long_noOut %>%
  group_by(Metric) %>%
  wilcox_effsize(Values ~ cluster)
print(effect_size_results_no_outliers)

# Plot with significance annotations
p <- ggplot(LateralizationDF_long_noOut, aes(x = cluster, y = Values, fill = Metric)) +
  geom_boxplot() +
  facet_wrap(~ Metric) +
  theme_minimal() +
  labs(title = "Comparison of LI-Metrics by Performance Group (Without Outliers)", y = "Value", x = "Performance Group")+
  ylim(-0.3, 0.4)

# Manually add significance and effect size annotations
p + 
  # Adding significance brackets and p-values
  
  geom_segment(data = subset(LateralizationDF_long_noOut, Metric == "LI-ERD"),aes(x = 1, xend = 2, y = 0.39, yend = 0.39), color = "black") +
  geom_segment(data = subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA"),aes(x = 1, xend = 2, y = 0.3, yend = 0.3), color = "black") +
  
  
  #geom_text(aes(x = 1.5, y = 5.1, label = "p = 6.3e-09"), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-ERD"),aes(x = 1.5, y = 0.39, label = "p < 2.2e-16"), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA"), aes(x = 1.5, y = 0.3, label = "p = 0.7496"), vjust = -0.5) +
  
  # Adding effect size labels for ERD
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-ERD"), aes(x = 1.5, y = 0.39, label = "Effect size: 0.38 (moderate)"), vjust = 1.5, size = 3, hjust = 0.5) +
  
  # Adding effect size labels for MFDFA
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA"), aes(x = 1.5, y = 0.3, label = "Effect size: 0.10 (small)"), vjust = 1.5, size = 3, hjust = 0.5)






kruskal_test_results <- LateralizationDF_long_noOut %>%
  group_by(cluster) %>%
  rstatix::kruskal_test(Values ~ Metric)
print(kruskal_test_results)

pairwise_comparisons <- function(df) {
  pairwise_results <- pairwise.wilcox.test(df$Values, df$Metric, p.adjust.method = "bonferroni")
  return(pairwise_results$p.value)
}

pairwise_results_no_outliers <- LateralizationDF_long_noOut %>%
  group_by(cluster) %>%
  do(tibble(pairwise_p_values = list(pairwise_comparisons(.))))
print(pairwise_results_no_outliers)

# Combine results for annotations
annotations <- pairwise_results_no_outliers %>%
  unnest(pairwise_p_values) %>%
  mutate(y.position = c(10))  # Adjust these positions based on your data range



p <- ggplot(LateralizationDF_long_noOut, aes(x = Metric, y = Values, fill = cluster)) +
  geom_boxplot() +
  facet_wrap(~ cluster) +
  theme_minimal() +
  labs(title = "Comparison of Performance Group by LI-Metrics (Without Outliers)", y = "Value", x = "Metric")
  ylim(-0.3, 0.3)


# Manually add significance and effect size annotations
p + 
  # Adding significance brackets and p-values
  
  geom_segment(data = subset(LateralizationDF_long_noOut, cluster == "Low"),aes(x = 1, xend = 2, y = 0.3, yend = 0.3), color = "black") +
  geom_segment(data = subset(LateralizationDF_long_noOut, cluster == "High"),aes(x = 1, xend = 2, y = 0.3, yend = 0.3), color = "black") +
  
  
  #geom_text(aes(x = 1.5, y = 5.1, label = "p = 6.3e-09"), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, cluster == "Low"),aes(x = 1.5, y = 0.3, label = "p = 0.224 "), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, cluster == "High"), aes(x = 1.5, y = 0.3, label = "p = 0.00000314"), vjust = -0.5) 

# Assuming LateralizationDF_long_noOut is your data frame

# Subset the data for LI-MFDFA and High performance group
mfd_high <- subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA" & cluster == "High")
mfd_low <- subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA" & cluster == "Low")
erd_high <- subset(LateralizationDF_long_noOut, Metric == "LI-ERD" & cluster == "High")
erd_low <- subset(LateralizationDF_long_noOut, Metric == "LI-ERD" & cluster == "Low")

# Perform the Wilcoxon signed-rank test against zero
wilcox_test_result <- wilcox.test(mfd_high$Values, mu = 0, exact = FALSE)
wilcox_test_result2 <- wilcox.test(mfd_low$Values, mu = 0, exact = FALSE)
wilcox_test_result3 <- wilcox.test(erd_high$Values, mu = 0, exact = FALSE)
wilcox_test_result4 <- wilcox.test(erd_low$Values, mu = 0, exact = FALSE)
# Print the results
print(wilcox_test_result)
print(wilcox_test_result2)
print(wilcox_test_result3)
print(wilcox_test_result4)



high_values <- LateralizationDF_long_noOut %>%
  filter(Metric == "LI-MFDFA" & cluster == "High") %>%
  pull(Values)

low_values <- LateralizationDF_long_noOut %>%
  filter(Metric == "LI-MFDFA" & cluster == "Low") %>%
  pull(Values)

# Perform the Wilcoxon rank-sum test
wilcox_test_result <- wilcox.test(high_values, low_values)

# Print the results
print(wilcox_test_result)



rstatix::wilcox_test(mfd_high$Values, mfd_low$Values)



```


















```{r}

con_C3 <- file("E:\\MonsterDataset\\TFR\\Baselined_C3_data.csv", open = "r")
con_C4 <- file("E:\\MonsterDataset\\TFR\\Baselined_C4_data.csv", open = "r")

# Loop over each trial
for (i in 1:3) {
  
  # Read one line/trial from the input files alternately
  trial_C3 <- as.numeric(strsplit(readLines(con_C3, n = 1, warn = FALSE), ",")[[1]])
  trial_C4 <- as.numeric(strsplit(readLines(con_C4, n = 1, warn = FALSE), ",")[[1]])

  print(length(trial_C3))
  scales <- logscale(scale_min=10, scale_max = length(trial_C3)/10, scale_ratio = 1.3)
  # Perform MFDFA on the current trials
  dfa_trial_C3 <- dfa(x = trial_C3,verbose = 0, order = 2, scales = scales, scale_ratio = 1.3)
  dfa_trial_C4 <- dfa(x = trial_C4,verbose = 0, order = 2, scales = scales, scale_ratio = 1.3)
  
  print(dfa_trial_C3)
  print(dfa_trial_C4)
}  
  

# Assuming trial_C3 is already loaded into your R environment
plot(trial_C3[2000:4000], type = 'l', main = "Line Plot of trial_C3", xlab = "Index", ylab = "Value",
     col = "blue", lwd = 2, ylim = c(-5, 15))  # 'l' specifies a line plot

```
```{r}
library(data.table)
library(foreach)
load_mfdfa <- function(index) {
  load(sprintf("E:\\MonsterDataset\\PreprocessedData\\mfdfa\\mfdfa_trial_C3_%d.Rdata", index))
  return(list(
    x = mf_dfa_trial_C3$x,
    order = mf_dfa_trial_C3$order,
    q = mf_dfa_trial_C3$q,
    scales = mf_dfa_trial_C3$scales,
    scale_ratio = mf_dfa_trial_C3$scale_ratio,
    log_scale = mf_dfa_trial_C3$log_scale,
    log_fq = mf_dfa_trial_C3$log_fq,
    Hq = mf_dfa_trial_C3$Hq,
    Tau = mf_dfa_trial_C3$Tau,
    h = mf_dfa_trial_C3$h,
    Dh = mf_dfa_trial_C3$Dh
  ))
}

ERD_DF_MFDFA <- ERD_data[ERD_data$Channel != "C4", ]
row.names(ERD_DF_MFDFA) <- NULL
ERD_DF_MFDFA$index <- seq_len(nrow(ERD_DF_MFDFA))


indexx <- 1
results_C3 <- list()
for (subject in unique(ERD_DF_MFDFA$Subject)) {
  for (session in unique(ERD_DF_MFDFA$Session)) {
    # Extract data for the current subject and session
    session_data <- ERD_DF_MFDFA[ERD_DF_MFDFA$Subject == subject & ERD_DF_MFDFA$Session == session, ]
    # Load MFDFA data for each trial using the index
    session_mfdfa <- lapply(session_data$index, load_mfdfa)

     if (length(session_mfdfa) > 0) {
      # Calculate means for each index and store results in a list
      session_results <- list()
      metrics <- c("x", "order", "q", "scales", "scale_ratio", "log_scale", "log_fq", "Hq", "Tau", "h", "Dh")
      for (metric in metrics) {
        max_length <- max(sapply(session_mfdfa, function(x) length(x[[metric]])))  # find the maximum length for the metric across all trials
        avg <- sapply(1:max_length, function(i) {
          vals <- sapply(session_mfdfa, function(x) if (i <= length(x[[metric]])) x[[metric]][i] else NA)
          mean(vals, na.rm = TRUE)
        })
        session_results[[metric]] <- avg
      }
      
      # Append to the results list
      results_C3[[indexx]] <- list(Subject = subject, Session = session, Data = session_results)
      indexx <- indexx + 1
    }
  }
}
save(results_C3, file= "E:\\MonsterDataset\\PreprocessedData\\mf_dfa_results_C3.Rdata")
```
```{r}
library(data.table)
library(foreach)

#function to load in MFDFA for each trial (using index, which is the index corresponding to the dataframe containing the metadata)
load_mfdfa <- function(index) {
  #Loads in the MFDFA results for 1 trial
  load(sprintf("E:\\MonsterDataset\\PreprocessedData\\mfdfa\\mfdfa_trial_C4_%d.Rdata", index))
  #Returns MFDFA as a list
  return(list(
    x = mf_dfa_trial_C4$x,
    order = mf_dfa_trial_C4$order,
    q = mf_dfa_trial_C4$q,
    scales = mf_dfa_trial_C4$scales,
    scale_ratio = mf_dfa_trial_C4$scale_ratio,
    log_scale = mf_dfa_trial_C4$log_scale,
    log_fq = mf_dfa_trial_C4$log_fq,
    Hq = mf_dfa_trial_C4$Hq,
    Tau = mf_dfa_trial_C4$Tau,
    h = mf_dfa_trial_C4$h,
    Dh = mf_dfa_trial_C4$Dh
  ))
}

ERD_DF_MFDFA <- ERD_data[ERD_data$Channel != "C3", ]
row.names(ERD_DF_MFDFA) <- NULL
ERD_DF_MFDFA$index <- seq_len(nrow(ERD_DF_MFDFA))


indexx <- 1
results_C4 <- list()
for (subject in unique(ERD_DF_MFDFA$Subject)) {
  for (session in unique(ERD_DF_MFDFA$Session)) {
    # Extract data for the current subject and session
    session_data <- ERD_DF_MFDFA[ERD_DF_MFDFA$Subject == subject & ERD_DF_MFDFA$Session == session, ]
    # Load MFDFA data for each trial using the index
    session_mfdfa <- lapply(session_data$index, load_mfdfa)

     if (length(session_mfdfa) > 0) {
      # Calculate means for each index and store results in a list
      session_results <- list()
      metrics <- c("x", "order", "q", "scales", "scale_ratio", "log_scale", "log_fq", "Hq", "Tau", "h", "Dh")
      for (metric in metrics) {
        max_length <- max(sapply(session_mfdfa, function(x) length(x[[metric]])))  # find the maximum length for the metric across all trials
        avg <- sapply(1:max_length, function(i) {
          vals <- sapply(session_mfdfa, function(x) if (i <= length(x[[metric]])) x[[metric]][i] else NA)
          mean(vals, na.rm = TRUE)
        })
        session_results[[metric]] <- avg
      }
      
      # Append to the results list
      results_C4[[indexx]] <- list(Subject = subject, Session = session, Data = session_results)
      indexx <- indexx + 1
    }
  }
}
save(results_C4, file= "E:\\MonsterDataset\\PreprocessedData\\mf_dfa_results_C4.Rdata")
```

#Discuss with Travis
#Show Multifractal Spectrum C4
```{r}
load_mfdfa <- function(index) {
  load(sprintf("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa\\mfdfa_trial_C4_%d.Rdata", index))
  load(sprintf("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa_rest\\mfdfa_trial_rest_C4_%d.Rdata", index))
  return(list(
    #x = mf_dfa_trial_C4$x,
    #order = mf_dfa_trial_C4$order,
    q = mf_dfa_trial_C4$q,
    #scales = mf_dfa_trial_C4$scales,
    #scale_ratio = mf_dfa_trial_C4$scale_ratio,
    #log_scale = mf_dfa_trial_C4$log_scale,
    #log_fq = mf_dfa_trial_C4$log_fq,
    Hq = mf_dfa_trial_C4$Hq,
    Tau = mf_dfa_trial_C4$Tau,
    h = mf_dfa_trial_C4$h,
    Dh = mf_dfa_trial_C4$Dh,
    
    Rq = mf_dfa_trial_rest_C4$q,
    RHq = mf_dfa_trial_rest_C4$Hq,
    RTau = mf_dfa_trial_rest_C4$Tau,
    Rh = mf_dfa_trial_rest_C4$h,
    RDh = mf_dfa_trial_rest_C4$Dh
    
  ))
}

ERD_DF_MFDFA <- ERD_data[ERD_data$Channel != "C3", ]
row.names(ERD_DF_MFDFA) <- NULL
ERD_DF_MFDFA$index <- seq_len(nrow(ERD_DF_MFDFA))

#xholder0 <- matrix(0, nrow = 7041, ncol = 1)
#orderholder0 <- 0
#qholder0 <- matrix(0, nrow = 21, ncol = 1)
#scalesholder0 <- matrix(0, nrow = 7, ncol = 1)
#scales_ratioholder0 <- 0
#log_scaleholder0 <- matrix(0, nrow = 7, ncol = 1)
#log_fqholder0 <- matrix(0, nrow = 7, ncol = 21)
#Hqholder0 <- matrix(0, nrow = 21, ncol = 1)
#Tauholder0 <- matrix(0, nrow = 21, ncol = 1)
hholder0 <- matrix(0, nrow = 20, ncol = 1)
Dholder0 <- matrix(0, nrow = 20, ncol = 1)
qholder0 <- matrix(0, nrow = 21, ncol = 1)
Hqholder0 <- matrix(0, nrow = 21, ncol = 1)
Tauholder0 <- matrix(0, nrow = 21, ncol = 1)

Rhholder0 <- matrix(0, nrow = 20, ncol = 1)
RDholder0 <- matrix(0, nrow = 20, ncol = 1)
Rqholder0 <- matrix(0, nrow = 21, ncol = 1)
RHqholder0 <- matrix(0, nrow = 21, ncol = 1)
RTauholder0 <- matrix(0, nrow = 21, ncol = 1)

hholder1 <- matrix(0, nrow = 20, ncol = 1)
Dholder1 <- matrix(0, nrow = 20, ncol = 1)
qholder1 <- matrix(0, nrow = 21, ncol = 1)
Hqholder1 <- matrix(0, nrow = 21, ncol = 1)
Tauholder1 <- matrix(0, nrow = 21, ncol = 1)

Rhholder1 <- matrix(0, nrow = 20, ncol = 1)
RDholder1 <- matrix(0, nrow = 20, ncol = 1)
Rqholder1 <- matrix(0, nrow = 21, ncol = 1)
RHqholder1 <- matrix(0, nrow = 21, ncol = 1)
RTauholder1 <- matrix(0, nrow = 21, ncol = 1)

# Loop through each forcedresult type
for (forced_result in c(0, 1)) {
  # Gather all data indices for the given forcedresult
  forced_data_indices <- ERD_DF_MFDFA$index[ERD_DF_MFDFA$forcedresult == forced_result]

  # Load MFDFA data for each index
  mfdfa_data <- lapply(forced_data_indices, load_mfdfa)

  # Calculate means for each index and store results
  if (length(mfdfa_data) > 0) {
    #metrics <- c("x", "order", "q", "scales", "scale_ratio", "log_scale", "log_fq", "Hq", "Tau", "h", "Dh")
    print(forced_result)
    }
    
    # Assign results to the appropriate list based on forced_result
    if (forced_result == 0) {
      zeroresult <<- length(mfdfa_data)
      sapply(mfdfa_data, function(x) {
        qholder0 <<- qholder0 + x[["q"]]
        Hqholder0 <<- Hqholder0 + x[["Hq"]]
        Tauholder0 <<- Tauholder0 + x[["Tau"]]
        hholder0 <<- hholder0 + x[["h"]]
        Dholder0 <<- Dholder0 + x[["Dh"]]
        
        Rqholder0 <<- Rqholder0 + x[["Rq"]]
        RHqholder0 <<- RHqholder0 + x[["RHq"]]
        RTauholder0 <<- RTauholder0 + x[["RTau"]]
        Rhholder0 <<- Rhholder0 + x[["Rh"]]
        RDholder0 <<- RDholder0 + x[["RDh"]]
        
        
      })
    } else {
      oneresult <<- length(mfdfa_data)
      sapply(mfdfa_data, function(x) {
        
        qholder1 <<- qholder1 + x[["q"]]
        Hqholder1 <<- Hqholder1 + x[["Hq"]]
        Tauholder1 <<- Tauholder1 + x[["Tau"]]
        hholder1 <<- hholder1 + x[["h"]]
        Dholder1 <<- Dholder1 + x[["Dh"]]
        
        Rqholder1 <<- Rqholder1 + x[["Rq"]]
        RHqholder1 <<- RHqholder1 + x[["RHq"]]
        RTauholder1 <<- RTauholder1 + x[["RTau"]]
        Rhholder1 <<- Rhholder1 + x[["Rh"]]
        RDholder1 <<- RDholder1 + x[["RDh"]]
        
      })
    }
}



save(hholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\hholder0_C4.Rdata")
save(Dholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Dholder0_C4.Rdata")
save(qholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\qholder0_C4.Rdata")
save(Hqholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Hqholder0_C4.Rdata")
save(Tauholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Tauholder0_C4.Rdata")

save(Rhholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rhholder0_C4.Rdata")
save(RDholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RDholder0_C4.Rdata")
save(Rqholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rqholder0_C4.Rdata")
save(RHqholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RHqholder0_C4.Rdata")
save(RTauholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RTauholder0_C4.Rdata")

save(hholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\hholder1_C4.Rdata")
save(Dholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Dholder1_C4.Rdata")
save(qholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\qholder1_C4.Rdata")
save(Hqholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Hqholder1_C4.Rdata")
save(Tauholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Tauholder1_C4.Rdata")

save(Rhholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rhholder1_C4.Rdata")
save(RDholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RDholder1_C4.Rdata")
save(Rqholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rqholder1_C4.Rdata")
save(RHqholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RHqholder1_C4.Rdata")
save(RTauholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RTauholder1_C4.Rdata")



hholder0 <- hholder0 / zeroresult
Dholder0 <- Dholder0 / zeroresult
qholder0 <- qholder0 / zeroresult
Hqholder0 <- Hqholder0 / zeroresult
Tauholder0 <- Tauholder0 / zeroresult

Rhholder0 <- Rhholder0 / zeroresult
RDholder0 <- RDholder0 / zeroresult
Rqholder0 <- Rqholder0 / zeroresult
RHqholder0 <- RHqholder0 / zeroresult
RTauholder0 <- RTauholder0 / zeroresult

hholder1 <-  hholder1 / oneresult
Dholder1 <-  Dholder1 / oneresult
qholder1 <-  qholder1 / oneresult
Hqholder1 <-  Hqholder1 / oneresult
Tauholder1 <-  Tauholder1 / oneresult

Rhholder1 <-  Rhholder1 / oneresult
RDholder1 <-  RDholder1 / oneresult
Rqholder1 <-  Rqholder1 / oneresult
RHqholder1 <-  RHqholder1 / oneresult
RTauholder1 <-  RTauholder1 / oneresult






# Create data frames for each dataset
data0 <- data.frame(h = hholder0, Dh = Dholder0, q = qholder0, Hq = Hqholder0, Tau = Tauholder0 ,Rh = Rhholder0, RDh = RDholder0, Rq = Rqholder0, RHq = RHqholder0, RTau = RTauholder0 , Set = "0")
data1 <- data.frame(h = hholder1, Dh = Dholder1, q = qholder1, Hq = Hqholder1, Tau = Tauholder1 ,Rh = Rhholder1, RDh = RDholder1, Rq = Rqholder1, RHq = RHqholder1, RTau = RTauholder1 , Set = "1")




```
```{r}
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\hholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Dholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\qholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Hqholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Tauholder0_C4.Rdata")

load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rhholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RDholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rqholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RHqholder0_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RTauholder0_C4.Rdata")

load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\hholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Dholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\qholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Hqholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Tauholder1_C4.Rdata")

load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rhholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RDholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rqholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RHqholder1_C4.Rdata")
load("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RTauholder1_C4.Rdata")

```
```{r}


# Assuming you have loaded these variables: hholder0, Dholder0, qholder0, Hqholder0, Tauholder0

# Calculate derivatives of tau(q) with respect to q
dq = diff(qholder0)  # Calculate the difference in q values
dtau = diff(Tauholder0)  # Calculate the difference in tau values
alpha = dtau / dq  # Equation 7: alpha as the derivative of tau with respect to q

# Implement Equation 8
f_alpha = alpha * qholder0[-length(qholder0)] - Tauholder0[-length(Tauholder0)]  # excluding the last element to match length of alpha

# For Equation 9, we need H'(q), assuming Hqholder0 holds H(q)
# First, calculate the derivative H'(q)
dHq = diff(Hqholder0) / dq
# Then use Equation 9
alpha_Hq = Hqholder0[-length(Hqholder0)] + qholder0[-length(qholder0)] * dHq  # again, excluding the last element

# Equation 10
f_alpha_Hq = qholder0[-length(qholder0)] * (alpha_Hq - Hqholder0[-length(Hqholder0)]) + 1

# Output the results or further processing
# For example, you can print or plot the results
print(list(alpha = alpha, f_alpha = f_alpha, alpha_Hq = alpha_Hq, f_alpha_Hq = f_alpha_Hq))

# Assuming alpha and f_alpha are calculated as shown previously
# Step 1: Calculate Δα and Δf
alpha_max <- max(alpha)
alpha_min <- min(alpha)
delta_alpha <- alpha_max - alpha_min  # Equation (11)

f_alpha_max <- f_alpha[which(alpha == alpha_max)]
f_alpha_min <- f_alpha[which(alpha == alpha_min)]
delta_f <- f_alpha_min - f_alpha_max  # Equation (12)

# Step 2: Fit the multifractal spectrum to a second-order polynomial
# Find alpha0 corresponding to the maximum f_alpha
alpha0 <- alpha[which.max(f_alpha)]
fit_coeffs <- lm(f_alpha ~ I((alpha - alpha0)^2) + I(alpha - alpha0))$coefficients
A <- fit_coeffs[2]  # Coefficient of (alpha-alpha0)^2
B <- fit_coeffs[3]  # Coefficient of (alpha-alpha0)
C <- fit_coeffs[1]  # Constant term

# Step 3: Plot the multifractal spectrum
plot(alpha, f_alpha, main = "Multifractal Spectrum", xlab = expression(alpha), ylab = expression(f(alpha)), pch = 19, ylim = c(-0.3, 1.4))

curve(A*(x - alpha0)^2 + B*(x - alpha0) + C, add = TRUE, col = 'red', lwd = 2)  # Add fitted curve

# Display results
print(paste("Delta alpha:", delta_alpha))
print(paste("Delta f:", delta_f))
print(paste("Coefficients of fitted polynomial - A:", A, "B:", B, "C:", C))

mf_spectrum <- data.frame(alpha = alpha, f_alpha = f_alpha)

# Calculate points for the fitted curve
fitted_points <- data.frame(alpha = seq(min(alpha), max(alpha), length.out = 100))
fitted_points$f_alpha <- A*(fitted_points$alpha - alpha0)^2 + B*(fitted_points$alpha - alpha0) + C


p <- ggplot() +
  geom_point(data = data, aes(x = h, y = Dh, color = Condition), size = 3) +
  geom_point(data = centralTendency, aes(x = midpoint, y = 0), shape = 4, size = 3) +
  geom_text(data = centralTendency, aes(x = midpoint, y = 0, label = sprintf("%.3f", midpoint), vjust = label_pos, color = Condition), size = 3.5) +
  scale_color_manual(values = color_values) +
  labs(x = "Singularity strength (h)", y = "D(h)") +
  ggtitle("Singularity Spectrum C4") +
  theme_minimal() +
  scale_y_continuous(limits = c(-0.2, 1), breaks = seq(-0.2, 1, by = 0.1)) +
  theme(legend.position = "right")

# Adding the multifractal spectrum in black
p <- p + geom_point(data = mf_spectrum, aes(x = alpha, y = f_alpha), color = "yellow") 
  #geom_line(data = fitted_points, aes(x = alpha, y = f_alpha), color = "black")
p
```

#Show Multifractal Spectrum plot C4
```{r}
library(dplyr)
Rdata0 <- data.frame(h = (Rhholder0), Dh = ( RDholder0), Condition = "Rest-unsuccesful")
Rdata1 <- data.frame(h = (Rhholder1), Dh = ( RDholder1), Condition = "Rest-succesful")

data0 <- data.frame(h = (hholder0), Dh = (Dholder0), Condition = "MI-unsuccesful")
data1 <- data.frame(h = (hholder1), Dh = (Dholder1), Condition = "MI-succesful")


DeltaDh0 <- max(Dholder0) - min(Dholder0)
DeltaDh1 <- max(Dholder1) - min(Dholder1)

DeltaRDh0 <- max(RDholder0) - min(RDholder0)
DeltaRDh1 <- max(RDholder1) - min(RDholder1)


cat("Delta Dh: unsuccsfulMI: ", DeltaDh0, "succsfulMI: " ,DeltaDh1,"unsuccsfulRest: ",DeltaRDh0, "succsfulRest: ", DeltaRDh1, "\n")

Deltah0 <- max(hholder0) - min(hholder0)
Deltah1 <- max(hholder1) - min(hholder1)

DeltaRh0 <- max(Rhholder0) - min(Rhholder0)
DeltaRh1 <- max(Rhholder1) - min(Rhholder1)
cat("Delta Dh: unsuccsfulMI: ", DeltaDh0, "succsfulMI: " ,DeltaDh1,"unsuccsfulRest: ",DeltaRDh0, "succsfulRest: ", DeltaRDh1, "\n")
cat("Delta h: unsuccsfulMI: ", Deltah0, "succsfulMI: " ,Deltah1,"unsuccsfulRest: ",DeltaRh0, "succsfulRest: ", DeltaRh1, "\n")

#difdata0 <- data.frame(h = (hholder0- Rhholder0), Dh = (Dholder0- RDholder0), Set = "dif-unsuccesful")
#difdata1 <- data.frame(h = (hholder1 - Rhholder1), Dh = (Dholder1- Dholder1), Set = "dif-succesful")


# Combine the data frames
data <- rbind(data0, data1, Rdata0, Rdata1)
colnames(data) <- c("h", "Dh", "Condition")
# Calculate the central tendency for each set
centralTendency <- data %>%
  group_by(Condition) %>%
  summarise(midpoint = (max(h) + min(h)) / 2)

# Print the results
print(centralTendency)

# Base plot
centralTendency$label_pos <- c(-1.5, -0.5, 1, 2)  # Adjust these values based on actual overlap seen in your plot

# Define the color mapping
color_values <- c("MI-unsuccesful" = "darkblue", "MI-succesful" = "lightblue", 
                  "Rest-unsuccesful" = "deeppink", "Rest-succesful" = "lightpink")

# Base plot
p <- ggplot(data, aes(x = h, y = Dh, color = Condition)) +
  geom_point(size = 2) +  # This adds the points to the plot
  scale_color_manual(values = color_values) +
  labs(x = "Singularity strength (h)", y = "D(h)") +
  ggtitle("Singularity Spectrum C4") +
  theme_minimal() +
  scale_y_continuous(limits = c(-0.2, 1), breaks = seq(-0.2, 1, by = 0.1)) +
  theme(legend.position = "right")

# Add central tendencies as points on the plot at y = 0
p <- p + geom_point(data = centralTendency, aes(x = midpoint, y = 0), shape = 4, size = 3)

# Add text labels for each central tendency with adjusted positions and matching colors
p <- p + geom_text(data = centralTendency, aes(x = midpoint, y = 0, label = sprintf("%.3f", midpoint), vjust = label_pos, color = Condition),
                   size = 3.5)

# Print the updated plot
print(p)
```
```{r}
Rdata0 <- data.frame(Hq = (RHqholder0), q = ( Rqholder0), Set = "Rest-unsuccesful")
Rdata1 <- data.frame(Hq = (RHqholder1), q = ( Rqholder1), Set = "Rest-succesful")

data0 <- data.frame(Hq = (Hqholder0), q = (qholder0), Set = "MI-unsuccesful")
data1 <- data.frame(Hq = (Hqholder1), q = (qholder1), Set = "MI-succesful")


DeltaHq0 <- max(Hqholder0) - min(Hqholder0)
DeltaHq1 <- max(Hqholder1) - min(Hqholder1)

DeltaRHq0 <- max(RHqholder0) - min(RHqholder0)
DeltaRHq1 <- max(RHqholder1) - min(RHqholder1)


cat("Delta Dh: unsuccsfulMI: ", DeltaHq0, "succsfulMI: " ,DeltaHq1,"unsuccsfulRest: ",DeltaRHq0, "succsfulRest: ", DeltaRHq1, "\n")



data <- rbind(data0, data1, Rdata0, Rdata1)
p <- ggplot(data, aes(x = q, y = Hq, color = Set)) +
  geom_point(size = 2) +  # This adds the points to the plot with a uniform shape but adjusts size
  scale_color_manual(values = c("MI-unsuccesful" = "darkblue", "MI-succesful" = "lightblue", 
                                "Rest-unsuccesful" = "deeppink", "Rest-succesful" = "lightpink")) + # Assign shades of colors
  labs(x = "Statistical moments (q)", y = "Generalized Hurst Exponents (H(q))") +
  ggtitle("Generalized Hurst over statistical moments C4") +
  theme_minimal() +
  scale_y_continuous(limits = c(0.9,2.7 ), breaks = seq(0.9,2.7, by = 0.1)) +
  geom_vline(xintercept = 2, linetype = "solid", color = "red", size = 1) +  # Vertical line at q=2
  geom_hline(yintercept = 1.190, linetype = "solid", color = "red", size = 1)+  # Horizontal line, adjust yintercept to match your specific needs
  geom_vline(xintercept = 2, linetype = "solid", color = "red", size = 1) +  # Vertical line at q=2
  geom_hline(yintercept = 1.330, linetype = "solid", color = "red", size = 1)  # Horizontal line, adjust yintercept to match your specific needs

  
# Print the plot
print(p)

```
```{r}


Rdata0 <- data.frame(Hq = (RTauholder0), q = ( Rqholder0), Set = "Rest-unsuccesful")
Rdata1 <- data.frame(Hq = (RTauholder1), q = ( Rqholder1), Set = "Rest-succesful")

data0 <- data.frame(Hq = (Tauholder0), q = (qholder0), Set = "MI-unsuccesful")
data1 <- data.frame(Hq = (Tauholder1), q = (qholder1), Set = "MI-succesful")

data <- rbind(data0, data1, Rdata0, Rdata1)
p <- ggplot(data, aes(x = q, y = Hq, color = Set)) +
  geom_point(size = 2) +  # This adds the points to the plot with a uniform shape but adjusts size
  scale_color_manual(values = c("MI-unsuccesful" = "darkblue", "MI-succesful" = "lightblue", 
                                "Rest-unsuccesful" = "deeppink", "Rest-succesful" = "lightpink")) + # Assign shades of colors
  labs(x = "Statistical moments (q)", y = "Tau (tau(q))") +
  ggtitle("Multifractal Spectrum C4") +
  theme_minimal() 
  
# Print the plot
print(p)

```
```{r}

```

#Show Multifractal Spectrum plot C3

```{r}
load_mfdfa <- function(index) {
  load(sprintf("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa\\mfdfa_trial_C3_%d.Rdata", index))
  load(sprintf("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa_rest\\mfdfa_trial_rest_C3_%d.Rdata", index))
  return(list(
    #x = mf_dfa_trial_C4$x,
    #order = mf_dfa_trial_C4$order,
    q = mf_dfa_trial_C3$q,
    #scales = mf_dfa_trial_C4$scales,
    #scale_ratio = mf_dfa_trial_C4$scale_ratio,
    #log_scale = mf_dfa_trial_C4$log_scale,
    #log_fq = mf_dfa_trial_C4$log_fq,
    Hq = mf_dfa_trial_C3$Hq,
    Tau = mf_dfa_trial_C3$Tau,
    h = mf_dfa_trial_C3$h,
    Dh = mf_dfa_trial_C3$Dh,
    
    Rq = mf_dfa_trial_rest_C3$q,
    RHq = mf_dfa_trial_rest_C3$Hq,
    RTau = mf_dfa_trial_rest_C3$Tau,
    Rh = mf_dfa_trial_rest_C3$h,
    RDh = mf_dfa_trial_rest_C3$Dh
    
  ))
}

ERD_DF_MFDFA <- ERD_data[ERD_data$Channel != "C4", ]
row.names(ERD_DF_MFDFA) <- NULL
ERD_DF_MFDFA$index <- seq_len(nrow(ERD_DF_MFDFA))

#xholder0 <- matrix(0, nrow = 7041, ncol = 1)
#orderholder0 <- 0
#qholder0 <- matrix(0, nrow = 21, ncol = 1)
#scalesholder0 <- matrix(0, nrow = 7, ncol = 1)
#scales_ratioholder0 <- 0
#log_scaleholder0 <- matrix(0, nrow = 7, ncol = 1)
#log_fqholder0 <- matrix(0, nrow = 7, ncol = 21)
#Hqholder0 <- matrix(0, nrow = 21, ncol = 1)
#Tauholder0 <- matrix(0, nrow = 21, ncol = 1)
hholder0 <- matrix(0, nrow = 20, ncol = 1)
Dholder0 <- matrix(0, nrow = 20, ncol = 1)
qholder0 <- matrix(0, nrow = 21, ncol = 1)
Hqholder0 <- matrix(0, nrow = 21, ncol = 1)
Tauholder0 <- matrix(0, nrow = 21, ncol = 1)

Rhholder0 <- matrix(0, nrow = 20, ncol = 1)
RDholder0 <- matrix(0, nrow = 20, ncol = 1)
Rqholder0 <- matrix(0, nrow = 21, ncol = 1)
RHqholder0 <- matrix(0, nrow = 21, ncol = 1)
RTauholder0 <- matrix(0, nrow = 21, ncol = 1)

hholder1 <- matrix(0, nrow = 20, ncol = 1)
Dholder1 <- matrix(0, nrow = 20, ncol = 1)
qholder1 <- matrix(0, nrow = 21, ncol = 1)
Hqholder1 <- matrix(0, nrow = 21, ncol = 1)
Tauholder1 <- matrix(0, nrow = 21, ncol = 1)

Rhholder1 <- matrix(0, nrow = 20, ncol = 1)
RDholder1 <- matrix(0, nrow = 20, ncol = 1)
Rqholder1 <- matrix(0, nrow = 21, ncol = 1)
RHqholder1 <- matrix(0, nrow = 21, ncol = 1)
RTauholder1 <- matrix(0, nrow = 21, ncol = 1)

# Loop through each forcedresult type
for (forced_result in c(0, 1)) {
  # Gather all data indices for the given forcedresult
  forced_data_indices <- ERD_DF_MFDFA$index[ERD_DF_MFDFA$forcedresult == forced_result]

  # Load MFDFA data for each index
  mfdfa_data <- lapply(forced_data_indices, load_mfdfa)

  # Calculate means for each index and store results
  if (length(mfdfa_data) > 0) {
    #metrics <- c("x", "order", "q", "scales", "scale_ratio", "log_scale", "log_fq", "Hq", "Tau", "h", "Dh")
    print(forced_result)
    }
    
    # Assign results to the appropriate list based on forced_result
    if (forced_result == 0) {
      zeroresult <<- length(mfdfa_data)
      sapply(mfdfa_data, function(x) {
        qholder0 <<- qholder0 + x[["q"]]
        Hqholder0 <<- Hqholder0 + x[["Hq"]]
        Tauholder0 <<- Tauholder0 + x[["Tau"]]
        hholder0 <<- hholder0 + x[["h"]]
        Dholder0 <<- Dholder0 + x[["Dh"]]
        
        Rqholder0 <<- Rqholder0 + x[["Rq"]]
        RHqholder0 <<- RHqholder0 + x[["RHq"]]
        RTauholder0 <<- RTauholder0 + x[["RTau"]]
        Rhholder0 <<- Rhholder0 + x[["Rh"]]
        RDholder0 <<- RDholder0 + x[["RDh"]]
        
        
      })
    } else {
      oneresult <<- length(mfdfa_data)
      sapply(mfdfa_data, function(x) {
        
        qholder1 <<- qholder1 + x[["q"]]
        Hqholder1 <<- Hqholder1 + x[["Hq"]]
        Tauholder1 <<- Tauholder1 + x[["Tau"]]
        hholder1 <<- hholder1 + x[["h"]]
        Dholder1 <<- Dholder1 + x[["Dh"]]
        
        Rqholder1 <<- Rqholder1 + x[["Rq"]]
        RHqholder1 <<- RHqholder1 + x[["RHq"]]
        RTauholder1 <<- RTauholder1 + x[["RTau"]]
        Rhholder1 <<- Rhholder1 + x[["Rh"]]
        RDholder1 <<- RDholder1 + x[["RDh"]]
        
      })
    }
}



save(hholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\hholder0_C4.Rdata")
save(Dholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Dholder0_C4.Rdata")
save(qholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\qholder0_C4.Rdata")
save(Hqholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Hqholder0_C4.Rdata")
save(Tauholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Tauholder0_C4.Rdata")

save(Rhholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rhholder0_C4.Rdata")
save(RDholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RDholder0_C4.Rdata")
save(Rqholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rqholder0_C4.Rdata")
save(RHqholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RHqholder0_C4.Rdata")
save(RTauholder0, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RTauholder0_C4.Rdata")

save(hholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\hholder1_C4.Rdata")
save(Dholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Dholder1_C4.Rdata")
save(qholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\qholder1_C4.Rdata")
save(Hqholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Hqholder1_C4.Rdata")
save(Tauholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Tauholder1_C4.Rdata")

save(Rhholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rhholder1_C4.Rdata")
save(RDholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RDholder1_C4.Rdata")
save(Rqholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\Rqholder1_C4.Rdata")
save(RHqholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RHqholder1_C4.Rdata")
save(RTauholder1, file= "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfaALL\\RTauholder1_C4.Rdata")



hholder0 <- hholder0 / zeroresult
Dholder0 <- Dholder0 / zeroresult
qholder0 <- qholder0 / zeroresult
Hqholder0 <- Hqholder0 / zeroresult
Tauholder0 <- Tauholder0 / zeroresult

Rhholder0 <- Rhholder0 / zeroresult
RDholder0 <- RDholder0 / zeroresult
Rqholder0 <- Rqholder0 / zeroresult
RHqholder0 <- RHqholder0 / zeroresult
RTauholder0 <- RTauholder0 / zeroresult

hholder1 <-  hholder1 / oneresult
Dholder1 <-  Dholder1 / oneresult
qholder1 <-  qholder1 / oneresult
Hqholder1 <-  Hqholder1 / oneresult
Tauholder1 <-  Tauholder1 / oneresult

Rhholder1 <-  Rhholder1 / oneresult
RDholder1 <-  RDholder1 / oneresult
Rqholder1 <-  Rqholder1 / oneresult
RHqholder1 <-  RHqholder1 / oneresult
RTauholder1 <-  RTauholder1 / oneresult






# Create data frames for each dataset
data0 <- data.frame(h = hholder0, Dh = Dholder0, q = qholder0, Hq = Hqholder0, Tau = Tauholder0 ,Rh = Rhholder0, RDh = RDholder0, Rq = Rqholder0, RHq = RHqholder0, RTau = RTauholder0 , Set = "0")
data1 <- data.frame(h = hholder1, Dh = Dholder1, q = qholder1, Hq = Hqholder1, Tau = Tauholder1 ,Rh = Rhholder1, RDh = RDholder1, Rq = Rqholder1, RHq = RHqholder1, RTau = RTauholder1 , Set = "1")



```










```{r}
mfdfa.plot(final_results$forced0, do.surrogate = TRUE)

mfdfa.plot(final_results$forced1, do.surrogate = TRUE)

```
```{r}
mfdfa.plot(final_results$forced0, do.surrogate = FALSE)

mfdfa.plot(final_results$forced1, do.surrogate = FALSE)

```
```{r}
load("E:\\MonsterDataset\\PreprocessedData\\mf_dfa_results_C3.Rdata")

```
```{r}
mf_dfa_sessions_C3
```

```{r}
pdf("All_Participants_MFDFA_Plots.pdf")

# Loop through the list of results and plot each one
for (i in seq_along(results_C3)) {
  # Extract the Data from the current list item
  current_data <- results_C3[[i]]$Data

  # Generate the MFDFA plot for the current participant's data
  # Ensure do.surrogate is set to FALSE as specified
  mfdfa.plot(current_data, do.surrogate = FALSE)
}

# Close the PDF device
dev.off()

```
```{r}
pdf("Participant1_MFDFA_WithSurogate_Plots.pdf")



for (session in unique(ERD_DF_MFDFA$Session)) {
  # Extract data for the current subject and session
  session_data <- ERD_DF_MFDFA[ERD_DF_MFDFA$Subject == 1 & ERD_DF_MFDFA$Session == session, ]
  # Load MFDFA data for each trial using the index
  session_mfdfa <- lapply(session_data$index, load_mfdfa)
  if (length(session_mfdfa) > 0) {
    for (trial in session_mfdfa){
      mfdfa.plot(trial, do.surrogate = TRUE)
    }
  }
}
dev.off()



pdf("Participant1_MFDFA_WithSurogate_Plots.pdf")
session_data <- ERD_DF_MFDFA[ERD_DF_MFDFA$Subject == 1 & ERD_DF_MFDFA$Session == 1, ]
  # Load MFDFA data for each trial using the index
  session_mfdfa <- lapply(session_data$index, load_mfdfa)
  if (length(session_mfdfa) > 0) {
    for (trial in session_mfdfa){
      mfdfa.plot(trial, do.surrogate = TRUE)
    }
  }
dev.off()
```

```{r}
mfdfa.plot(results_C3[[10]]$Data, do.surrogate = FALSE)

```
```{r}
results_C3[[119]]$Data


results_C3[[2]]$Data

```


```{r}

for (i in seq_along(final_results)) {
  # Assuming your data is stored in results[[1]]$Data$log_fq
  original_data <- final_results[[i]]$log_fq
  
  # Calculate the number of rows and columns in the new matrix
  num_rows <- length(original_data) / 7
  num_columns <- 7
  
  # Reshape the data into the desired format
  reshaped_data <- matrix(original_data, nrow = num_columns, ncol = num_rows, byrow = TRUE)
  final_results[[i]]$log_fq <- reshaped_data
  
  
}



```
```{r}
# Load necessary library
library(dplyr)



# Assuming ERD_data_New2 is your dataframe and it has columns 'Subject' and 'ERD'
ERD_data_New2 <- ERD_data_New2 %>%
  group_by(Subject) %>%
  mutate(ERD_normalized_participant = scale(ERD, center = TRUE, scale = TRUE)) %>%
  ungroup()  # Remove the grouping specification

# View the updated dataframe to confirm the new column
head(ERD_data_New2)




# Load necessary library
library(dplyr)

# Assuming ERD_data_New2 is your dataframe and it has columns 'Subject', 'Session', and 'ERD'
ERD_data_New2 <- ERD_data_New2 %>%
  group_by(Session) %>%
  mutate(ERD_normalized_session = scale(ERD, center = TRUE, scale = TRUE)) %>%
  ungroup()  # Remove the grouping specification

# View the updated dataframe to confirm the new column
head(ERD_data_New2)


# Load necessary library
library(dplyr)

# Assuming ERD_data_New2 is your dataframe and it has columns 'Subject', 'Session', and 'ERD'
ERD_data_New2 <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  mutate(ERD_normalized_participant_session = scale(ERD_normalized_participant, center = TRUE, scale = TRUE)) %>%
  ungroup()  # Remove the grouping specification

# View the updated dataframe to confirm the new column
head(ERD_data_New2)

```
```{r}
scales <- logscale(scale_min=8, scale_max = 1000, scale_ratio = 1.3)


mf.dfa.test <- mfdfa(x = Raw_data[[1]], q = c(-5:15), order = 1, scales = scales, scale_ratio = 1.3)

mf.dfa.test.W <- max(mf.dfa.test$h) - min(mf.dfa.test$h)
```
```{r}
length(Raw_data)
```
```{r}
print(mf.dfa.test.W)
mfdfa.plot(mf.dfa.test, do.surrogate = "true")
```





#Loops over all trials and calculates MFDFA for each of them while also storing the full MFDFA output
```{r}

scales <- logscale(scale_min=10, scale_max = 312, scale_ratio = 1.3)
# File path for the MFDFA widths data
widths_file_path <- "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mf_dfa_widths.csv"

# Ensure the files are empty and set up with the correct headers
write.table(x = data.frame(mf_dfa_width = numeric(0)), file = widths_file_path, sep = ",", col.names = TRUE, row.names = FALSE)
#write.table(x = data.frame(index = integer(), mf_dfa_trial_W = double()), file = results_file_path, sep = ",", col.names = TRUE, row.names = FALSE)

# Open the input files for reading
con_C3 <- file("E:\\MonsterDataset\\PreprocessedData\\Baselined_C3_data.csv", open = "r")
con_C4 <- file("E:\\MonsterDataset\\PreprocessedData\\Baselined_C4_data.csv", open = "r")

# Loop over each trial
for (i in 1:97666) {
  # Read one line/trial from the input files alternately
  trial_C3 <- as.numeric(strsplit(readLines(con_C3, n = 1, warn = FALSE), ",")[[1]])
  trial_C4 <- as.numeric(strsplit(readLines(con_C4, n = 1, warn = FALSE), ",")[[1]])

  # Perform MFDFA on the current trials
  mf_dfa_trial_C3 <- mfdfa(x = trial_C3, q = c(-5:15), order = 2, scales = scales, scale_ratio = 1.3)
  mf_dfa_trial_C4 <- mfdfa(x = trial_C4, q = c(-5:15), order = 2, scales = scales, scale_ratio = 1.3)
  
  

  
  # It's crucial to ensure that none of these fields contain commas to avoid breaking the CSV structure
  # Create a single data frame to store
  
  
  
  
  
  save(mf_dfa_trial_C3, file = paste0("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa\\mfdfa_trial_C3_", i, ".RData"))
  save(mf_dfa_trial_C4, file = paste0("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa\\mfdfa_trial_C4_", i, ".RData"))


  # Calculate and store the widths
  mf_dfa_width_C3 <- max(mf_dfa_trial_C3$h) - min(mf_dfa_trial_C3$h)
  mf_dfa_width_C4 <- max(mf_dfa_trial_C4$h) - min(mf_dfa_trial_C4$h)

  # Write widths to the widths file
  write.table(x = data.frame(mf_dfa_width_C3), file = widths_file_path, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE)
  write.table(x = data.frame(mf_dfa_width_C4), file = widths_file_path, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE)

  # Store the current trial's output in the results file
  #mf_dfa_trial_W_C3 <- data.frame(index = i, mf_dfa_trial_W = mf_dfa_width_C3)
  #mf_dfa_trial_W_C4 <- data.frame(index = i, mf_dfa_trial_W = mf_dfa_width_C4)

  
  
}

# Close the connections
close(con_C3)
close(con_C4)


```

#MFDFA REST
```{r}
# #Loops over all trials and calculates MFDFA for each of them while also storing the full MFDFA output
# 
# mf_dfa_trials_list <- list()
# 
# #existing scales definition
# scales <- logscale(scale_min=8, scale_max = 2000, scale_ratio = 1.5)
# 
# # Initializes an empty dataframe for storing index and mf_dfa_trial_W
# results_df <- data.frame(index=integer(), mf_dfa_trial_W=double())
# 
# for (i in 1:104872) {
#   mf.dfa.trial <- mfdfa(x = Raw_data[[i]], q = c(-5:15), order = 2, scales = scales, scale_ratio = 1.5)
#   mf.dfa.trial.W <- max(mf.dfa.trial$h) - min(mf.dfa.trial$h)
#   
#   # Store the current trial's output in the list
#   mf_dfa_trials_list[[i]] <- mf.dfa.trial
# 
#   # Append the new result to the dataframe
#   results_df <- rbind(results_df, data.frame(index = i, mf_dfa_trial_W = mf.dfa.trial.W))
# }
# 
# # Now, mf_dfa_trials_list contains all the mf.dfa.trial outputs
# 
# #Also stores the spectrum width
# mf_dfa_widths_trials_list <- list()
# 
# for (i in 1:104872) {
#   mf_dfa_widths_trials_list[[i]] <- max(mf_dfa_trials_list[[i]]$h) - min(mf_dfa_trials_list[[i]]$h)
# }

scales <- logscale(scale_min=10, scale_max = 200, scale_ratio = 1.3)
# File path for the MFDFA widths data
widths_file_path <- "E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mf_dfa_widths_rest.csv"

# Ensure the files are empty and set up with the correct headers
write.table(x = data.frame(mf_dfa_width = numeric(0)), file = widths_file_path, sep = ",", col.names = TRUE, row.names = FALSE)
#write.table(x = data.frame(index = integer(), mf_dfa_trial_W = double()), file = results_file_path, sep = ",", col.names = TRUE, row.names = FALSE)

# Open the input files for reading
con_C3 <- file("E:\\MonsterDataset\\PreprocessedData\\Raw_Rest_C3_data.csv", open = "r")
con_C4 <- file("E:\\MonsterDataset\\PreprocessedData\\Raw_Rest_C4_data.csv", open = "r")

# Loop over each trial
for (i in 1:97666) {
  # Read one line/trial from the input files alternately
  trial_C3 <- as.numeric(strsplit(readLines(con_C3, n = 1, warn = FALSE), ",")[[1]])
  trial_C4 <- as.numeric(strsplit(readLines(con_C4, n = 1, warn = FALSE), ",")[[1]])

  # Perform MFDFA on the current trials
  mf_dfa_trial_rest_C3 <- mfdfa(x = trial_C3, q = c(-5:15), order = 2, scales = scales, scale_ratio = 1.3)
  mf_dfa_trial_rest_C4 <- mfdfa(x = trial_C4, q = c(-5:15), order = 2, scales = scales, scale_ratio = 1.3)
  
  

  
  # It's crucial to ensure that none of these fields contain commas to avoid breaking the CSV structure
  # Create a single data frame to store
  
  
  
  
  
  save(mf_dfa_trial_rest_C3, file = paste0("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa_rest\\mfdfa_trial_rest_C3_", i, ".RData"))
  save(mf_dfa_trial_rest_C4, file = paste0("E:\\MonsterDataset\\PreprocessedData\\BetterValues\\mfdfa_rest\\mfdfa_trial_rest_C4_", i, ".RData"))


  # Calculate and store the widths
  mf_dfa_width_rest_C3 <- max(mf_dfa_trial_rest_C3$h) - min(mf_dfa_trial_rest_C3$h)
  mf_dfa_width_rest_C4 <- max(mf_dfa_trial_rest_C4$h) - min(mf_dfa_trial_rest_C4$h)

  # Write widths to the widths file
  write.table(x = data.frame(mf_dfa_width_rest_C3), file = widths_file_path, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE)
  write.table(x = data.frame(mf_dfa_width_rest_C4), file = widths_file_path, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE)

  # Store the current trial's output in the results file
  #mf_dfa_trial_W_C3 <- data.frame(index = i, mf_dfa_trial_W = mf_dfa_width_C3)
  #mf_dfa_trial_W_C4 <- data.frame(index = i, mf_dfa_trial_W = mf_dfa_width_C4)

  
  
}

# Close the connections
close(con_C3)
close(con_C4)


```









```{r}

load_trial_data <- function(trial_index) {
  # Construct the file paths
  
  # Return as a list
  #list(C3 = mf_dfa_trial_C3, C4 = mf_dfa_trial_C4)
}

# Example of how to use the function
#trial_data <- load_trial_data(4) # Loads the data for trial index 3
trial_index <- 5
file_path_C3 <- paste0("E:\\MonsterDataset\\PreprocessedData\\mfdfa\\mfdfa_trial_C3_", trial_index, ".RData")
file_path_C4 <- paste0("E:\\MonsterDataset\\PreprocessedData\\mfdfa\\mfdfa_trial_C4_", trial_index, ".RData")

  # Load the data
load(file_path_C3)
load(file_path_C4)

  
  
print(mf_dfa_trial_C3) # Print data for C3
print(mf_dfa_trial_C4) # Print data for C4


```
```{r}
con <- file("E:\\MonsterDataset\\PreprocessedData\\Baselined_C3_data.csv", open = "r")
line_count <- length(readLines(con))
close(con)
print(line_count)


```
```{r}
mfdfa.plot(mf_dfa_trial_C3, do.surrogate = "true")

```





#Show violin plot of MFDFA widths by task status
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Remove outliers based on the 1.5*IQR rule for mf_dfa_widths
ERD_data_cleaned <- ERD_data_New2 %>%
  group_by(`Session`) %>%
  filter(mf_dfa_widths >= quantile(mf_dfa_widths, 0.25) - 1.5 * IQR(mf_dfa_widths) & 
         mf_dfa_widths <= quantile(mf_dfa_widths, 0.75) + 1.5 * IQR(mf_dfa_widths))

# Perform the Wilcoxon rank-sum test with cleaned data
test_result <- wilcox.test((mf_dfa_widths-mf_dfa_rest_widths) ~ `OnTask`, data = ERD_data_cleaned)

# Create a violin plot for mf_dfa_widths by task status with outliers removed
p <- ggplot(ERD_data_cleaned, aes(x = `OnTask`, y = (mf_dfa_widths-mf_dfa_rest_widths), fill = `OnTask`)) +
  geom_violin(trim = FALSE) +
  labs(title = "MFDFA Width by Task Status (Outliers Removed)",
       subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)),
       x = "Task Status",
       y = "MFDFA Width") +
  theme_minimal() +
  scale_fill_manual(values = c("on-task" = "blue", "off-task" = "red"))

# Print the plot
print(p)


```

```{r}
# Load the necessary library
library(dplyr)
library(dplyr)

# Assuming your data is in a DataFrame called 'df'
# Here we need to know the specific names of your columns that indicate movement side

# Add a column to categorize movement side relative to channel
Lateralization <- ERD_data_cleaned %>%
  mutate(MovementSide = ifelse(targetnumber == "1", "Left", "Right"))

# Calculate the Power differences and LI
results <- Lateralization %>%
  group_by(Subject, Session, Trial) %>%
  summarize(
    PowerC3Left = sum(ERD[Channel == "C3" & MovementSide == "Left"]),
    PowerC4Left = sum(ERD[Channel == "C4" & MovementSide == "Left"]),
    PowerC3Right = sum(ERD[Channel == "C3" & MovementSide == "Right"]),
    PowerC4Right = sum(ERD[Channel == "C4" & MovementSide == "Right"]),
    .groups = 'drop'
  ) %>%
  mutate(
    Left_diff = PowerC3Left - PowerC4Left,
    Right_diff = PowerC4Right - PowerC3Right,
    LI = (Left_diff + Right_diff) / 2
  )

# Display the results
print(results)



```
```{r}

# Now, plot and analyze BAI
library(ggplot2)

# Plotting BAI by Participant and Session
p_bai <- ggplot(BAI_data, aes(x = Subject, y = BAI, fill = Session)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Bilateral Activation Index by Participant and Session",
       x = "Participant",
       y = "Mean BAI") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")

print(p_bai)

# Descriptive statistics for BAI
summary_stats <- summary(BAI_data$BAI)
print(summary_stats)

# Histogram of BAI distribution
p_bai_dist <- ggplot(BAI_data, aes(x = BAI)) +
  geom_histogram(bins = 20, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Bilateral Activation Index",
       x = "Bilateral Activation Index",
       y = "Frequency") +
  theme_minimal()

print(p_bai_dist)

```
```{r}
# Load the dplyr library
library(dplyr)

# Assume your data is loaded into a DataFrame called 'df'
# df <- read.csv("your_data.csv") # Uncomment this if you need to load data from a CSV file

# Process the DataFrame
results <- ERD_data_cleaned %>%
  # Group data by Trial, Subject, and Session to ensure each group has one on-task and one off-task
  group_by(Trial, Subject, Session) %>%
  # Assuming that each group will have exactly one 'on-task' and one 'off-task' row
  summarize(
    ERD_OnTask = sum(ERD[OnTask == "on-task"]),
    ERD_OffTask = sum(ERD[OnTask == "off-task"]),
    .groups = 'drop'  # Drop the grouping after summarization
  ) %>%
  # Calculate BAi
  mutate(BAi = abs(ERD_OnTask - ERD_OffTask) / (abs(ERD_OnTask) + abs(ERD_OffTask)))

# Print the results or write them to a new CSV
print(results)
# write.csv(results, "results.csv") # Uncomment to save the results to a CSV file

# Now, plot and analyze BAI
results$Session <- as.factor(results$Session)

# Now plot using ggplot
library(ggplot2)

p_bai <- ggplot(results, aes(x = Subject, y = BAi, fill = Session)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Bilateral Activation Index by Participant and Session",
       x = "Participant",
       y = "Mean BAI") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")

print(p_bai)

# Descriptive statistics for BAI
summary_stats <- summary(results$BAi)
print(summary_stats)

# Histogram of BAI distribution
p_bai_dist <- ggplot(results, aes(x = BAi)) +
  geom_histogram(bins = 40, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Bilateral Activation Index",
       x = "Bilateral Activation Index",
       y = "Frequency") +
  theme_minimal()

print(p_bai_dist)

```
#Show violin plot for ERD by task status
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Remove outliers based on the 1.5*IQR rule

ERD_data_cleaned <- ERD_data_New2 %>%
  group_by(`Session`) %>%
  mutate(
    Lower_Bound = quantile(ERD, 0.25) - 1.5 * IQR(ERD),
    Upper_Bound = quantile(ERD, 0.75) + 1.5 * IQR(ERD)
  ) %>%
  filter(ERD >= Lower_Bound & ERD <= Upper_Bound) %>%
  select(-Lower_Bound, -Upper_Bound)

ERD_data_cleaned
# Create a violin plot for ERD_normalized by task status with outliers removed
p <- ggplot(ERD_data_cleaned, aes(x = `OnTask`, y = ERD, fill = `OnTask`)) +
  geom_violin(trim = FALSE) +
  labs(title = "ERD_normalized by Task Status (Outliers Removed)",
       x = "Task Status",
       y = "ERD_normalized_participant_session") +
  theme_minimal() +
  scale_fill_manual(values = c("on-task" = "blue", "off-task" = "red"))

# Print the plot
print(p)


# Perform the Wilcoxon rank-sum test with cleaned data
test_result <- wilcox.test(ERD ~ `OnTask`, data = ERD_data_cleaned)

# Update plot with test results
p + labs(subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)))

```

```{r}


```


#Show violin plot of Regression by task status
```{r}
library(ggplot2)
library(dplyr)

# Remove outliers based on the 1.5*IQR rule for mfdfa_width
ERD_data_cleaned <- ERD_data_New2 %>%
  group_by(`Session`) %>%
  filter(combined_slopes_C3_C4  >= quantile(combined_slopes_C3_C4 , 0.25) - 1.5 * IQR(combined_slopes_C3_C4 ) & 
         combined_slopes_C3_C4  <= quantile(combined_slopes_C3_C4 , 0.75) + 1.5 * IQR(combined_slopes_C3_C4 ))

# Perform the Wilcoxon rank-sum test with cleaned data
test_result <- wilcox.test(combined_slopes_C3_C4  ~ `OnTask`, data = ERD_data_cleaned)

# Create a violin plot for slopes by task status with outliers removed
p <- ggplot(ERD_data_cleaned, aes(x = `OnTask`, y = combined_slopes_C3_C4 , fill = `OnTask`)) +
  geom_violin(trim = FALSE) +
  labs(title = "PSD slopes by Task Status (Outliers Removed)",
       subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)),
       x = "Task Status",
       y = "slopes") +
  theme_minimal() +
  scale_fill_manual(values = c("on-task" = "blue", "off-task" = "red"))

# Print the plot
print(p)

```


#Plotting MFDFA over sessions
```{r}



# Assuming your data frame is named ERD_data_New2 and includes 'mf_dfa_widths', 'Session', and 'Subject' columns


# Step 2: Calculate mean normalized mf_dfa_widths per participant per session
mfdfa_summary <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(mean_normalized_mfdfa = mean(mf_dfa_widths, na.rm = TRUE)) %>%
  ungroup()

# Convert 'Session' to a factor if it's not already
mfdfa_summary$Session <- as.factor(mfdfa_summary$Session)

# Step 3: Create the boxplot of mean normalized mf_dfa_widths values
ggplot(mfdfa_summary, aes(x = Session, y = mean_normalized_mfdfa, fill = Session)) +
  geom_boxplot() +
  labs(title = "Mean Normalized Boxplots of mf_dfa_widths by Session",
       x = "Session",
       y = "Mean Normalized mf_dfa_widths") +
  theme_minimal() +
  theme(legend.position = "none")  # Optional: Remove the legend if not needed


``` 

```{r}





# Assuming your data frame is named ERD_data_New2 and it includes 'ERD', 'Session', and 'Subject' columns


# Step 2: Calculate mean normalized ERD per participant per session
ERD_summary <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(mean_normalized_ERD = mean(ERD, na.rm = TRUE)) %>%
  ungroup()

# Convert 'Session' to a factor if it's not already
ERD_summary$Session <- as.factor(ERD_summary$Session)

# Step 3: Create the boxplot of mean normalized ERD values
ggplot(ERD_summary, aes(x = Session, y = mean_normalized_ERD, fill = Session)) +
  geom_boxplot() +
  coord_cartesian(ylim = c(-0.3, 2)) +
  labs(title = "Mean Normalized Boxplots of ERD by Session",
       x = "Session",
       y = "Mean Normalized ERD") +
  theme_minimal() +
  theme(legend.position = "none")  # Optional: Remove the legend if not needed

``` 


```{r}



# Assuming your data frame is named ERD_data_New2 and includes 'combined_slopes_C3_C4 ', 'Session', and 'Subject' columns
slopes_summary <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(mean_normalized_slopes = mean(combined_slopes_C3_C4, na.rm = TRUE)) %>%
  ungroup()



# Convert 'Session' to a factor if it's not already
slopes_summary$Session <- as.factor(slopes_summary$Session)

# Step 3: Create the boxplot of mean normalized combined_slopes_C3_C4  values
ggplot(slopes_summary, aes(x = Session, y = mean_normalized_slopes, fill = Session)) +
  geom_boxplot() +
  labs(title = "Mean Normalized Boxplots of combined_slopes_C3_C4  by Session",
       x = "Session",
       y = "Mean Normalized combined_slopes_C3_C4 ") +
  theme_minimal() +
  theme(legend.position = "none")  # Optional: Remove the legend if not needed


``` 




#Performance Spectral slopes

```{r}



# Remove outliers based on the 1.5*IQR rule for mfdfa_width
ERD_data_cleaned <- ERD_data_New2 %>%
  group_by(`Session`) %>%
  filter(combined_slopes_C3_C4  >= quantile(combined_slopes_C3_C4 , 0.25) - 1.5 * IQR(combined_slopes_C3_C4 ) & 
         combined_slopes_C3_C4  <= quantile(combined_slopes_C3_C4 , 0.75) + 1.5 * IQR(combined_slopes_C3_C4 ))

ERD_data_cleaned$forcedresult <- ifelse(ERD_data_cleaned$forcedresult >= 1, "successful", "unsuccessful")


test_result <- wilcox.test(combined_slopes_C3_C4  ~ `forcedresult`, data = ERD_data_cleaned)

# Create a violin plot for slopes by task status with outliers removed
p <- ggplot(ERD_data_cleaned, aes(x = `forcedresult`, y = combined_slopes_C3_C4 , fill = `forcedresult`)) +
  geom_violin(trim = FALSE) +
  labs(title = "PSD slopes by Task Status (Outliers Removed)",
       subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)),
       x = "Task Status",
       y = "slopes") +
  theme_minimal() +
  scale_fill_manual(values = c("unsuccessful" = "red", "successful" = "green"))


# Perform the Wilcoxon rank-sum test with cleaned data

# Update plot with test results
p + labs(subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)))

``` 
```{r}



# Remove outliers based on the 1.5*IQR rule for mfdfa_width
ERD_data_cleaned <- ERD_data_New2 %>%
  group_by(`Session`) %>%
  filter(mf_dfa_widths  >= quantile(mf_dfa_widths , 0.25) - 1.5 * IQR(mf_dfa_widths ) & 
         mf_dfa_widths  <= quantile(mf_dfa_widths , 0.75) + 1.5 * IQR(mf_dfa_widths ))

ERD_data_cleaned$forcedresult <- ifelse(ERD_data_cleaned$forcedresult >= 1, "successful", "unsuccessful")



test_result <- wilcox.test((mf_dfa_widths- mf_dfa_rest_widths)  ~ `forcedresult`, data = ERD_data_cleaned)
# Create a violin plot for slopes by task status with outliers removed
p <- ggplot(ERD_data_cleaned, aes(x = `forcedresult`, y = (mf_dfa_widths- mf_dfa_rest_widths) , fill = `forcedresult`)) +
  geom_violin(trim = FALSE) +
  labs(title = "MFDFA widths by Performance (Outliers Removed)",
       subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)),
       x = "Task Status",
       y = "slopes") +
  theme_minimal() +
  scale_fill_manual(values = c("unsuccessful" = "red", "successful" = "green"))


# Perform the Wilcoxon rank-sum test with cleaned data

# Update plot with test results
p + labs(subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)))

``` 

```{r}



# Remove outliers based on the 1.5*IQR rule for mfdfa_width
ERD_data_cleaned <- ERD_data_New2 %>%
  group_by(`Session`) %>%
  filter(ERD  >= quantile(ERD , 0.25) - 1.5 * IQR(ERD ) & 
         ERD  <= quantile(ERD , 0.75) + 1.5 * IQR(ERD ))

ERD_data_cleaned$forcedresult <- ifelse(ERD_data_cleaned$forcedresult >= 1, "successful", "unsuccessful")




# Create a violin plot for slopes by task status with outliers removed
p <- ggplot(ERD_data_cleaned, aes(x = `forcedresult`, y = ERD , fill = `forcedresult`)) +
  geom_violin(trim = FALSE) +
  labs(title = "ERD by Performance (Outliers Removed)",
       subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)),
       x = "Task Status",
       y = "slopes") +
  theme_minimal() +
  scale_fill_manual(values = c("unsuccessful" = "red", "successful" = "green"))


# Perform the Wilcoxon rank-sum test with cleaned data
test_result <- wilcox.test(ERD  ~ `forcedresult`, data = ERD_data_cleaned)
# Update plot with test results
p + labs(subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)))

``` 


#comparing mfdfa MI vs mfdfa rest
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming ERD_data_New2 is already loaded. If not, load it.
# ERD_data_New2 <- read.csv("path_to_file.csv")

# Prepare data for plotting
long_data <- ERD_data_New2 %>%
  pivot_longer(cols = c(mf_dfa_widths, mf_dfa_rest_widths), names_to = "condition", values_to = "value")

# Create violin plot
p <- ggplot(long_data, aes(x = condition, y = value, fill = condition)) +
  geom_violin(trim = FALSE) +
  labs(title = "Violin Plot of MF DFA Widths", x = "Condition", y = "Values") +
  theme_minimal()

# Perform Wilcoxon test directly on vectors
test_result <- wilcox.test(ERD_data_New2$mf_dfa_widths, ERD_data_New2$mf_dfa_rest_widths, paired = TRUE)

# Add p-value to the plot
p <- p + annotate("text", x = 1.5, y = max(long_data$value, na.rm = TRUE), 
                  label = paste("Wilcoxon test p-value:", format(test_result$p.value)), color = "red")

# Print the plot
print(p)

```


```{r}
library("performance")
ERD_data_New2
ERD_data_New2 <- ERD_data_New2 %>%
  mutate(mfdfa_diff = mf_dfa_widths - mf_dfa_rest_widths)


model <- lmer(mfdfa_diff ~ forcedresult * OnTask + (1|Subject), data = ERD_data_New2)

model2 <- lmer(mfdfa_diff ~ forcedresult:OnTask + (1|Subject), data = ERD_data_New2)
summary(model)
summary(model2)




model_performance(model)
model_performance(model2)

compare_performance(model, model2)
compare_performance(model2)
```
```{r}
# Model for ERD
model_ERD <- lmer(ERD ~ poly(Trials, 18) + (1|Subject), data = ERD_data_New2)
summary(model_ERD)


# Model for MFDFA Widths
model_mfdfa <- lmer(mf_dfa_widths ~ poly(Trials, 18) + (1|Subject), data = ERD_data_New2)
summary(model_mfdfa)

# Generate a new data frame for predictions
trial_range <- range(ERD_data_New2$Trials)
new_data <- data.frame(Trials = seq(min(ERD_data_New2$Trials), max(ERD_data_New2$Trials), by = 1))


# Predict values
new_data$predicted_ERD <- predict(model_ERD, newdata = new_data, re.form = NA)
new_data$predicted_mfdfa_widths <- predict(model_mfdfa, newdata = new_data, re.form = NA)

# Normalize by converting to z-scores
new_data$normalized_ERD <- scale(new_data$predicted_ERD)
new_data$normalized_mfdfa_widths <- scale(new_data$predicted_mfdfa_widths)


library(ggplot2)

ggplot() +
  geom_line(data = new_data, aes(x = Trials, y = normalized_ERD, colour = "ERD")) +
  geom_line(data = new_data, aes(x = Trials, y = normalized_mfdfa_widths, colour = "MFDFA Widths")) +
  labs(title = "Effect of Trial Number on Normalized ERD and MFDFA Widths",
       x = "Trial Number",
       y = "Normalized Predicted Values") +
  scale_colour_manual(name = "Measure", values = c("ERD" = "blue", "MFDFA Widths" = "red")) +
  theme_minimal() +
  theme(legend.title = element_text(color = "black", size = 10),
        legend.text = element_text(color = "black", size = 10))

```
```{r}
model_interaction <- lmer(mfdfa_diff ~ poly(Trials, 6) * forcedresult + (1|Subject), data = ERD_data_New2)
summary(model_interaction)
# Generate a data frame for predictions that spans the range of trials
trial_range <- range(ERD_data_New2$Trials)
new_data <- expand.grid(Trials = seq(trial_range[1], trial_range[2], by = 1),
                        forcedresult = factor(c(0, 1)))

# Predict mfdfa_diff using the interaction model
new_data$predicted_mfdfa_diff <- predict(model_interaction, newdata = new_data, re.form = NA)  # Exclude random effects

# Plotting
library(ggplot2)
ggplot(new_data, aes(x = Trials, y = predicted_mfdfa_diff, color = forcedresult, group = forcedresult)) +
  geom_line() +
  labs(title = "Interaction of Trial Progression and Performance on MFDFA Values",
       x = "Trial Number",
       y = "Predicted MFDFA Difference",
       color = "Performance Status") +
  theme_minimal()



```
#Clustering based on performance
```{r}
performance <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(accuracy = mean(forcedresult))



# Load necessary library
library(cluster)

# K-means clustering into 2 groups
set.seed(123) # for reproducibility
kmeans_result <- kmeans(performance$accuracy, centers = 2)

# Add cluster results to the dataframe
performance$cluster <- kmeans_result$cluster

# To ensure consistent labeling, you might want to relabel the clusters based on mean accuracy
cluster_labels <- aggregate(accuracy ~ cluster, performance, mean)
names(cluster_labels) <- c('cluster', 'mean_accuracy')
cluster_labels <- cluster_labels[order(-cluster_labels$mean_accuracy),]
performance$cluster <- factor(performance$cluster, levels = cluster_labels$cluster, labels = c('High', 'Low'))


library(ggplot2)

# Plotting the accuracies with cluster labels
ggplot(performance, aes(x = Subject, y = accuracy, color = cluster)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Participant Accuracies by Cluster", x = "Subject", y = "Accuracy")

# Plotting a histogram or boxplot to visualize distribution
ggplot(performance, aes(x = cluster, y = accuracy, fill = cluster)) +
  geom_boxplot() +
  labs(title = "Distribution of Accuracies by Performance Cluster", x = "Cluster", y = "Accuracy") +
  theme_minimal()



```
#Statistical test between mfdfa values of high and low performers
```{r}
summed_mfdfa <- ERD_data_New2 %>%
  group_by(Subject) %>%
  summarise(mfdfavalues = mean(mf_dfa_widths))# - mean(mf_dfa_rest_widths))


performance_mfdfa <- merge(summed_mfdfa, performance, by = "Subject")



shapiro_low <- shapiro.test(performance_mfdfa$mfdfavalues[performance_mfdfa$cluster == 'Low'])
shapiro_high <- shapiro.test(performance_mfdfa$mfdfavalues[performance_mfdfa$cluster == 'High'])

# Print normality test results
print(shapiro_low)
print(shapiro_high)

ks_test <- ks.test(performance_mfdfa$mfdfavalues[performance_mfdfa$cluster == 'Low'],
                   performance_mfdfa$mfdfavalues[performance_mfdfa$cluster == 'High'])

# Print KS test results
print(ks_test)


library(ggplot2)

# Boxplot to see the distribution of MFDFA widths for each group
p <- ggplot(performance_mfdfa, aes(x = cluster, y = mfdfavalues, fill = cluster)) +
    geom_violin(trim = FALSE) +
    labs(title = "Distribution of MFDFA Widths by Performance Group", x = "Performance Group", y = "MFDFA Width") +
    theme_minimal()

test_result <- wilcox.test(mfdfavalues  ~ `cluster`, data = performance_mfdfa)
# Update plot with test results
p + labs(subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)))



# Histogram for a more detailed distribution view
ggplot(performance_mfdfa, aes(x = mfdfavalues, fill = cluster)) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 30) +
  facet_wrap(~cluster) +
  labs(title = "Histogram of MFDFA Widths by Performance Group", x = "MFDFA Width", y = "Count")



library(ggplot2)

# Boxplot of MFDFA widths
ggplot(performance_mfdfa, aes(x = cluster, y = mfdfavalues, fill = cluster)) +
  geom_boxplot() +
  labs(title = "Boxplot of MFDFA Widths by Performance Group", x = "Performance Group", y = "MFDFA Width") +
  theme_minimal()

# Histogram of MFDFA widths
ggplot(performance_mfdfa, aes(x = mfdfavalues, fill = cluster)) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 30) +
  facet_wrap(~cluster) +
  labs(title = "Histogram of MFDFA Widths by Performance Group", x = "MFDFA Width", y = "Density") +
  theme_minimal()




```

#THIS IS WHERE I WAS
```{r}


performance <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(accuracy = mean(forcedresult))

summed_mfdfa <- ERD_data_New2 %>%
  group_by(Subject,Session) %>%
  summarise(mfdfavalues = mean(mf_dfa_widths))# 

summed_mfdfa_dif <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(mfdfa_dif_values = (mean(mf_dfa_widths)- mean(mf_dfa_rest_widths)))# 

summed_ERD <- ERD_data_New2 %>%
  group_by(Subject,Session) %>%
  summarise(ERDvalues = mean(ERD))# 

performance_mfdfa <- merge(summed_mfdfa, performance)
performance_mfdfa <- merge(performance_mfdfa, summed_mfdfa_dif)
performance_mfdfa <- merge(performance_mfdfa, summed_ERD)

performance <- merge(performance, power_wide)
performance["mfdfa_dif_values"] <- performance_mfdfa$mfdfa_dif_values

performance_mfdfa <- merge(performance_mfdfa, mfdfa_wide)



kmeans_result <- kmeans(performance$accuracy, centers = 2)

# Add cluster results to the dataframe
performance$cluster <- kmeans_result$cluster

# To ensure consistent labeling, you might want to relabel the clusters based on mean accuracy
cluster_labels <- aggregate(accuracy ~ cluster, performance, mean)
names(cluster_labels) <- c('cluster', 'mean_accuracy')
cluster_labels <- cluster_labels[order(-cluster_labels$mean_accuracy),]
performance$cluster <- factor(performance$cluster, levels = cluster_labels$cluster, labels = c('High', 'Low'))



#performance_mfdfa <- merge(performance_mfdfa, power_wide)
#shapiro.test(performance_mfdfa$mfdfavalues)
#shapiro.test(performance_mfdfa$mfdfa_dif_values)
#shapiro.test(performance_mfdfa$accuracy)


# Pearson correlation
# Pearson's correlation test
#pearson_test <- cor.test(performance_mfdfa$mfdfavalues, performance_mfdfa$accuracy, method = "pearson")
#print(pearson_test)

#pearson_test <- cor.test(performance_mfdfa$mfdfa_dif_values, performance_mfdfa$accuracy, method = "pearson")
#print(pearson_test)

#pearson_test <- cor.test(performance_mfdfa$ERDvalues, performance_mfdfa$accuracy, method = "pearson")
#print(pearson_test)


# Spearman's correlation test
#spearman_test <- cor.test(performance_mfdfa$mfdfavalues, performance_mfdfa$accuracy, method = "spearman")
#print(spearman_test)

# Kendall's correlation test
#kendall_test <- cor.test(performance_mfdfa$mfdfavalues, performance_mfdfa$accuracy, method = "kendall")
#print(kendall_test)



# Scatter plot with a smooth line
ggplot(performance_mfdfa, aes(x = mfdfavalues, y = accuracy)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)


# Scatter plot with a smooth line
ggplot(performance_mfdfa, aes(x = mfdfa_dif_values, y = accuracy)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)


# Assuming performance_mfdfa and performance are dataframes in R
# First, create a dataframe with both datasets
# Load necessary libraries
library(tidyr)

performance_mfdfa["cluster"] <- performance$cluster
LateralizationDF <- data.frame(LImfdfa = performance_mfdfa$LI_mfdfa, LIERD = performance$LI, accuracy = performance_mfdfa$accuracy, cluster = performance$cluster)

# Reshape the dataframe to a long format suitable for comparison
LateralizationDF_long <- pivot_longer(LateralizationDF, 
                                      cols = c(LImfdfa, LIERD), 
                                      names_to = "Condition", 
                                      values_to = "Values")

# Perform the Wilcoxon test on the reshaped data
test_result <- wilcox.test(Values ~ Condition, data = LateralizationDF_long)

# Create a boxplot
p <- ggplot(LateralizationDF_long, aes(x = Condition, y = Values, fill = Condition)) +
  geom_boxplot(outlier.shape = NA) +  # No outlier points will be drawn
  labs(title = "Comparison of Lateralization Indices",
       x = "Condition",
       y = "Lateralization Index") +
  theme_minimal() +
  coord_cartesian(ylim = c(-5, 5))  # Adjust these values as necessary

# Add the significance level to the plot
p <- p + geom_text(aes(x = 1.5, y = 5, label = paste("p-value:", format.pval(test_result$p.value, digits = 3))),
                   vjust = -0.5, color = "red")

p


model1 <- lmer(mfdfa_dif_values ~ accuracy + LI + (1|Subject), data = performance)
model2 <- lmer(mfdfa_dif_values ~ accuracy + LI_mfdfa + (1|Subject), data = performance_mfdfa)
summary(model1)
summary(model2)
coef(model2)

```

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyr)
library(dplyr)

# Create a dataframe with necessary variables and factor the cluster into low and high groups
LateralizationDF <- data.frame(
  LImfdfa = performance_mfdfa$LI_mfdfa, 
  LIERD = performance$LI,
  Subject = performance$Subject,
  cluster = factor(performance$cluster, levels = c("Low", "High"))
)

# Reshape the dataframe to a long format suitable for comparison
LateralizationDF_long <- pivot_longer(LateralizationDF, 
                                      cols = c(LImfdfa, LIERD), 
                                      names_to = "Condition", 
                                      values_to = "Values") %>%
  mutate(Condition = interaction(Condition, cluster, sep = "_"))

# Separate data by condition for statistical testing
groups <- split(LateralizationDF_long, LateralizationDF_long$Condition)

# Perform the Mann-Whitney U test for each pair
# Perform Kruskal-Wallis test
kw_test_result <- kruskal.test(Values ~ Condition, data = LateralizationDF_long)

p <- ggplot(LateralizationDF_long, aes(x = Condition, y = Values, fill = Condition)) +
  geom_boxplot(outlier.shape = NA) +
  labs(title = "Comparison of Lateralization Indices by Condition and Performance Level",
       x = "Group",
       y = "Lateralization Index") +
  coord_cartesian(ylim = c(-5, 5))+
  theme_minimal()

# Add the Kruskal-Wallis test result to the plot
p <- p + geom_text(aes(x = 2, y = Inf, label = paste("Kruskal-Wallis p value:", format.pval(kw_test_result$p.value, digits = 3))),
                   hjust = 0.5, vjust = 1.5, color = "red")

# Print the pl
p
```

#Lateralization by performance
```{r}
library(multcomp)
library(multcompView)


library(FSA) # For Dunn's test
library(rstatix) # For Wilcoxon tests and effect sizes
library(coin)





metrics <- rep(c("LI-MFDFA", "LI-ERD"), length.out = 868)
LateralizationDF_long["Metric"] <- metrics

LateralizationDF_long <- LateralizationDF_long %>%
  mutate(PairID = rep(1:(n()/2), each = 2))


identify_outliers <- function(df) {
  Q1 <- quantile(df$Values, 0.25)
  Q3 <- quantile(df$Values, 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Identify outliers in the data
  df$Outlier <- df$Values < lower_bound | df$Values > upper_bound
  
  # If either in a pair is an outlier, mark both
  df$Outlier <- ave(df$Outlier, df$PairID, FUN = any)
  
  return(df)
}

# Apply function to dataset
LateralizationDF_long <- LateralizationDF_long %>%
  group_by(cluster) %>%
  do(identify_outliers(.))

# Remove outliers
LateralizationDF_long_noOut <- LateralizationDF_long %>%
  filter(!Outlier)





descriptive_stats <- LateralizationDF_long_noOut %>%
  group_by(Metric, cluster) %>%
  summarise(
    count = n(),
    mean = mean(Values, na.rm = TRUE),
    sd = sd(Values, na.rm = TRUE)
  )
print(descriptive_stats)

shapiro_test_results <- LateralizationDF_long_noOut %>%
  group_by(Metric, cluster) %>%
  summarise(p_value = shapiro.test(Values)$p.value)
print(shapiro_test_results)

kruskal_test_results <- LateralizationDF_long_noOut %>%
  group_by(Metric) %>%
  rstatix::kruskal_test(Values ~ cluster)
print(kruskal_test_results)

pairwise_comparisons <- function(df) {
  pairwise_results <- pairwise.wilcox.test(df$Values, df$cluster, p.adjust.method = "bonferroni")
  return(pairwise_results$p.value)
}

pairwise_results_no_outliers <- LateralizationDF_long_noOut %>%
  group_by(Metric) %>%
  do(tibble(pairwise_p_values = list(pairwise_comparisons(.))))
print(pairwise_results_no_outliers)

# Combine results for annotations
annotations <- pairwise_results_no_outliers %>%
  unnest(pairwise_p_values) %>%
  mutate(y.position = c(10))  # Adjust these positions based on your data range


# Effect size calculation
effect_size_results_no_outliers <- LateralizationDF_long_noOut %>%
  group_by(Metric) %>%
  wilcox_effsize(Values ~ cluster)
print(effect_size_results_no_outliers)

# Plot with significance annotations
p <- ggplot(LateralizationDF_long_noOut, aes(x = cluster, y = Values, fill = Metric)) +
  geom_boxplot() +
  facet_wrap(~ Metric, scales = "free") +
  theme_minimal() +
  labs(title = "Comparison of LI-Metrics by Performance Group (Without Outliers)", y = "Value", x = "Performance Group")+
  ylim(-5, 5)

# Manually add significance and effect size annotations
p + 
  # Adding significance brackets and p-values
  
  geom_segment(data = subset(LateralizationDF_long_noOut, Metric == "LI-ERD"),aes(x = 1, xend = 2, y = 4.9, yend = 4.9), color = "black") +
  geom_segment(data = subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA"),aes(x = 1, xend = 2, y = 4.9, yend = 4.9), color = "black") +
  
  
  #geom_text(aes(x = 1.5, y = 5.1, label = "p = 6.3e-09"), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-ERD"),aes(x = 1.5, y = 4.9, label = "p = 6.3e-09"), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA"), aes(x = 1.5, y = 4.9, label = "p = 0.4583417"), vjust = -0.5) +
  
  # Adding effect size labels for ERD
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-ERD"), aes(x = 1.5, y = 5, label = "Effect size: 0.38 (moderate)"), vjust = 1.5, size = 3, hjust = 0.5) +
  
  # Adding effect size labels for MFDFA
  geom_text(data = subset(LateralizationDF_long_noOut, Metric == "LI-MFDFA"), aes(x = 1.5, y = 5, label = "Effect size: 0.10 (small)"), vjust = 1.5, size = 3, hjust = 0.5)






kruskal_test_results <- LateralizationDF_long_noOut %>%
  group_by(cluster) %>%
  rstatix::kruskal_test(Values ~ Metric)
print(kruskal_test_results)

pairwise_comparisons <- function(df) {
  pairwise_results <- pairwise.wilcox.test(df$Values, df$Metric, p.adjust.method = "bonferroni")
  return(pairwise_results$p.value)
}

pairwise_results_no_outliers <- LateralizationDF_long_noOut %>%
  group_by(cluster) %>%
  do(tibble(pairwise_p_values = list(pairwise_comparisons(.))))
print(pairwise_results_no_outliers)

# Combine results for annotations
annotations <- pairwise_results_no_outliers %>%
  unnest(pairwise_p_values) %>%
  mutate(y.position = c(10))  # Adjust these positions based on your data range



p <- ggplot(LateralizationDF_long_noOut, aes(x = Metric, y = Values, fill = cluster)) +
  geom_boxplot() +
  facet_wrap(~ cluster, scales = "free") +
  theme_minimal() +
  labs(title = "Comparison of Performance Group by LI-Metrics (Without Outliers)", y = "Value", x = "Metric")+
  ylim(-5, 5)

# Manually add significance and effect size annotations
p + 
  # Adding significance brackets and p-values
  
  geom_segment(data = subset(LateralizationDF_long_noOut, cluster == "Low"),aes(x = 1, xend = 2, y = 4.9, yend = 4.9), color = "black") +
  geom_segment(data = subset(LateralizationDF_long_noOut, cluster == "High"),aes(x = 1, xend = 2, y = 4.9, yend = 4.9), color = "black") +
  
  
  #geom_text(aes(x = 1.5, y = 5.1, label = "p = 6.3e-09"), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, cluster == "Low"),aes(x = 1.5, y = 4.9, label = "p = 0.224 "), vjust = -0.5) +
  
  geom_text(data = subset(LateralizationDF_long_noOut, cluster == "High"), aes(x = 1.5, y = 4.9, label = "p = 0.00000314"), vjust = -0.5) 


```

```{r}
if (!requireNamespace("dunn.test", quietly = TRUE)) {
    install.packages("dunn.test")
}
library(dunn.test)
# Dunn's test for multiple comparisons
dunn_result <- dunn.test(x =LateralizationDF_long$Values, g =LateralizationDF_long$Condition, method = "bonferroni")

# Create a boxplot
p <- ggplot(LateralizationDF_long, aes(x = Condition, y = Values, fill = Condition)) +
  geom_boxplot(outlier.shape = NA) +
  labs(title = "Comparison of Lateralization Indices by Condition and Performance Level",
       x = "Group",
       y = "Lateralization Index") +
  theme_minimal()

# Add Dunn's test results to the plot
# We filter to significant results only for adding text
significant_comparisons <- dunn_result$comparisons[dunn_result$P.adjusted  < 0.05]
labels <- paste(significant_comparisons, "p =", signif(dunn_result$P.adjusted [dunn_result$P.adjusted < 0.05], 3))

p <- p + geom_text(aes(x = 2, y = Inf, label = paste(labels, collapse = "\n")),
                   hjust = 0.5, vjust = 1.5, color = "red")

# Print the plot
print(p)




```

#Correlation Lateralization and performance
```{r}
mfdfa_wide


correlationLIP <- merge(mfdfa_wide[, !(names(mfdfa_wide) %in% c("C31", "C32", "C41", "C42"))], performance)
correlationLIP <- merge(correlationLIP, power_wide[, !(names(power_wide) %in% c("C31", "C32", "C41", "C42"))])


pearson_test <- cor.test( correlationLIP$accuracy,correlationLIP$LI, method = "pearson")
print(pearson_test)

ggplot(correlationLIP, aes(x = accuracy, y = LI)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)




```
#Lateralization
```{r}

# Create new dataframes excluding specified columns
mfdfa_w_s["LIIndex"] <- mfdfa_wide[, !(names(mfdfa_wide) %in% c("C31", "C32", "C41", "C42", "Subject", "Session"))]
mfdfa_w_s <- mfdfa_w_s[, !(names(mfdfa_w_s) %in% c("LI_mfdfa"))]

power_w_s["LIIndex"] <- power_wide[, !(names(power_wide) %in% c("C31", "C32", "C41", "C42",  "Subject", "Session"))]
power_w_s <- power_w_s[, !(names(power_w_s) %in% c("LI"))]

# Add subject and session columns
mfdfa_w_s$Type <- "MFDF"
power_w_s$Type <- "Power"

# Combine the dataframes
merged_df <- data.frame(
  mfdfa_w_s = mfdfa_w_s$LIIndex,
  power_w_s = power_w_s$LIIndex
)

# Assign values to the columns
combined_df <- rbind(mfdfa_w_s, power_w_s)

# Reshape data for plotting
#combined_df_long <- gather(combined_df, variable, value, -Type, -subject, -session)

# Plot violin plot
p<- ggplot(combined_df, aes(x = Type, y = LIIndex, fill = Type)) +
    geom_violin(trim = FALSE) +
    labs(x = "Variable", y = "Value", title = "Violin Plot of MFDF and Power") +
    theme_minimal()


test_result <- wilcox.test(merged_df$mfdfa_w_s, merged_df$power_w_s, paired = TRUE)
posthoc <- pairwise.wilcox.test(merged_df$mfdfa_w_s, merged_df$power_w_s, paired = TRUE, p.adjust.method = "bonferroni")


# Add p-value to the plot
p <- p + annotate("text", x = 1.5, y = max(long_data$value, na.rm = TRUE), 
                  label = paste("Wilcoxon test p-value:", format(test_result$p.value)), color = "red")

# Print the plot
print(p)


```
























```{r}
library("performance")
library("patchwork")
model_transformed <- lmer(mf_dfa_widths ~  forcedresult*Session + On-task + (1|Subject) , 
                           data = ERD_data_New2)
summary(model_transformed)

coef(model_transformed)
#, family = binomial


check_model(model_transformed)


#I want to check how the mfdfa widths change based on whether they are on-task, considering that each subject might have a different MFDFA value. I also wish to investigate whether this change in mfdfa is different when 
model_transformed <- lmer(mf_dfa_widths ~  forcedresult*Session + (1|Subject) , 
                           data = ERD_data_New2)

#I also wish to see whether the difference between rest and MI is different based on whether they were successful or not. I'm also wondering whether this difference is impacted on whether they are on-task or not. . 
model_transformed <- lmer(mf_dfa_widths ~  forcedresult*Session  + (1|Subject) , 
                           data = ERD_data_New2)


model_transformed <- lmer(mfdfa_dif ~  forcedresult + Session + (1|Subject) , 
                           data = ERD_data_New2)

model_transformed
```

#A different question would be whether contra lateraization happens in mfdfa values.

#Discuss with Travis: Made a few Linear Mixed Models: help with interpretation
#ERD not significant, meaning..? Lower or higher ERD is not significant?
```{r}
ERDModel_FR <- lmer(forcedresult ~ ERD+ (1|Trial) + 
                           (1 | Channel:OnTask) + (1 | Subject:Session), 
                           data = ERD_data_New2)
summary(ERDModel_FR)

```
#Subject:Session has a variance of 0.0198 and a standard deviation of 0.1407.
#Trial has a variance of 0.02435 and a standard deviation of 0.1560.
#Channel:OnTask has a very small variance (0.00003714), suggesting minimal variability at this level.
#The Residual variance is 0.2035 with a standard deviation of 0.4511, indicating the variability in forcedresult not explained by the model.

#The intercept (mean value of forcedresult when ERD is zero) is estimated at 0.5238 with a standard error of 0.01172, highly significant (p < 0.0001).
#The coefficient for ERD is -5.690e-04 with a standard error of 3.438e-04. It is statistically insignificant (p = 0.0979), indicating that it is not sure whether there is an effect of ERD on the prediction (what??? Does this mean the average decrease isn't good enough)

```{r}
install.packages("performance")
library("performance")
model_transformed <- glmer(forcedresult ~ mf_dfa_widths+ (1|Trial) + 
                           (1 | Channel:OnTask) + (1 | Subject:Session), family = binomial, 
                           data = ERD_data_New2)
summary(model_transformed)

#Check model assumptions
check_model(model_transformed)
```
#Subject:Session has a variance of 0.0198 and a standard deviation of 0.1407.
#Trial has a variance of 0.02436 and a standard deviation of 0.1561.
#Channel:OnTask has a very small variance (0.00003673), suggesting minimal variability at this level.
#The Residual variance is 0.2035 with a standard deviation of 0.4511, indicating the variability in forcedresult not explained by the model.

#The intercept (mean value of forcedresult when mf_dfa_widths is zero) is estimated at 0.5505 with a standard error of 0.01353, highly significant (p < 0.0001).
#The coefficient for mf_dfa_widths is -0.01618 with a standard error of 0.004109. It is statistically significant (p = 0.0000823), indicating that mf_dfa_widths has a negative effect on forcedresult. The negative coefficient suggests that as mf_dfa_widths increases, the forcedresult decreases.

#The correlation between the intercept and mf_dfa_widths is -0.501, indicating a moderate inverse relationship between the intercept and the slope of mf_dfa_widths.







#Confirm Effect size of ERD with LMM
```{r}
library(lme4)

# Model for ERD with random intercepts for participants and sessions
# Assuming 'On_task' is coded as a factor in your data
erd_model <- lmer(ERD ~ OnTask + (1 | Subject) + (1 | Subject:Session), data = ERD_data_New2)
summary(erd_model)


# Model for MFDFA Width with random intercepts for participants and sessions
mfdfa_model <- lmer(mfdfa_width ~ OnTask + (1 | Subject) + (1 | Subject:Session), data = ERD_data_New2)
summary(mfdfa_model)


```
#Calculate the standard Deviation of MFDFA
```{r}
# Calculate standard deviation and range
std_dev_mf_dfa_widths <- sd(ERD_data_New2$mf_dfa_widths)
range_mf_dfa_widths <- range(ERD_data_New2$mf_dfa_widths)

# Print the results
print(paste("Standard Deviation of mf_dfa_widths:", std_dev_mf_dfa_widths))
print(paste("Range of mf_dfa_widths:", range_mf_dfa_widths))


```
```{r}
# Applying a transformation if needed (e.g., log transformation)
# Check for zero or negative values before applying log transformation
ERD_data_New2$mf_dfa_widths_log <- log(ERD_data_New2$mf_dfa_widths + 1)

# Scaling predictors
ERD_data_New2$forcedresult_scaled <- scale(ERD_data_New2$forcedresult)
ERD_data_New2$Trial_scaled <- scale(ERD_data_New2$Trial)

# Fit the model with transformed and scaled variables
library(lme4)
model_transformed <- lmer(mf_dfa_widths_log ~ forcedresult_scaled + Trial_scaled + 
                           (1 | Channel:OnTask) + (1 | Subject:Session), 
                           data = ERD_data_New2)
summary(model_transformed)


```


```{r}
AIC(model_transformed,model_transformed_ERD)

```
```{r}
# Load necessary package
library(lme4)

# Fit your model (assuming it has been fitted and is named 'model_transformed')
model_transformed <- lmer(mf_dfa_widths_log ~ forcedresult_scaled + Trial_scaled + 
                          (1 | Channel:OnTask) + (1 | Subject:Session), 
                          data = ERD_data_New2)

# Plot residuals vs fitted values
plot(fitted(model_transformed), residuals(model_transformed),
     xlab = "Fitted Values", ylab = "Residuals", main = "Residuals vs Fitted Values")
abline(h = 0, col = "red")

```





```{r}
mfdfa_model <- lmer(mf_dfa_widths ~ Trial + forcedresult  + (1 | Channel:OnTask) +  (1 | Subject) +(1 |Session), data = ERD_data_New2)
summary(mfdfa_model)

```









```{r}

# Standardize ERD
ERD_data_New2$ERD_z <- scale(ERD_data_New2$ERD)

# Calculate the IQR for the standardized ERD
iqr_erd_z <- IQR(ERD_data_New2$ERD_z)

# Calculate the lower and upper bounds for what is considered not an outlier
lower_bound_z <- quantile(ERD_data_New2$ERD_z, 0.25) - 1.5 * iqr_erd_z
upper_bound_z <- quantile(ERD_data_New2$ERD_z, 0.75) + 1.5 * iqr_erd_z

# Filter out the outliers based on the IQR rule for the standardized data
ERD_data_NoOutliers_z <- ERD_data_New2 %>%
  filter(ERD_z >= lower_bound_z & ERD_z <= upper_bound_z)

# Fit the null and full model without outliers on the standardized ERD
erd_model_null_NoOutliers_z <- lmer(ERD_z ~ (1 | Subject) + (1 | Session), data = ERD_data_NoOutliers_z)
erd_model_NoOutliers_z <- lmer(ERD_z ~ OnTask + (1 | Subject) + (1 | Session), data = ERD_data_NoOutliers_z)

# Compare the models with ANOVA
anova_result_z <- anova(erd_model_null_NoOutliers_z, erd_model_NoOutliers_z)

# Get summary for fixed effects of the full model without outliers on standardized ERD
summary_erd_no_outliers_z <- summary(erd_model_NoOutliers_z)

# Calculating and printing the estimated coefficients with confidence intervals without outliers on standardized ERD
confint_erd_no_outliers_z <- confint(erd_model_NoOutliers_z, level = 0.95)

# Output the results
print(anova_result_z)
print(summary_erd_no_outliers_z)
print(confint_erd_no_outliers_z)


```

#Effect of OnTask on ERD: The OnTask variable is statistically significant (p = 0.000125). The estimated coefficient for OnTask is −0.0004637, which suggests that being on-task is associated with a decrease in ERD values compared to being off-task. The confidence interval for this estimate ([−0.0007006,−0.0002268]) does not include zero, further supporting the significance of the effect.

#Magnitude of the Effect: Although the effect size is small, its statistical significance suggests that the task status (on-task vs. off-task) does have a measurable impact on ERD. This effect, while minor, is congruent with previous findings of a decrease in ERD in the active brain side. Since the effect size is quite small, it indicates that there is only minimal lateralization.

#esiduals Analysis: The scaled residuals of the model range from −2.8095 to 2.8592 with a median very close to zero, indicating that the residuals are well distributed around the median and the model does not suffer from any obvious non-linear biases or heteroscedasticity issues.

#Random Effects: The variance components for random effects show that there is variability both within subjects across sessions and between subjects, which the model accounts for effectively. This supports the reliability of the fixed effects estimation.


#To further interpret this effect size, we can consider Cohen's conventions for small, medium, and large effects, which are 0.2, 0.5, and 0.8 standard deviations, respectively. By these standards, an effect size of -0.01365 (this value being based on a standardized LMM model) would be considered very small, suggesting that while the effect is statistically significant, its practical significance might be limited. This would need to be considered in the context of the domain of study—some fields or applications may find small effect sizes to be meaningful, especially in large samples or where even small changes are important.




#QQ plot
```{r}
# Q-Q plot of residuals
qqnorm(residuals(erd_model_NoOutliers_z))
qqline(residuals(erd_model_NoOutliers_z), col = "red")


```
```{r}
cat("\nRandom Effects Summary:\n")
print(ranef(erd_model_NoOutliers_z, condVar = TRUE))
```
#MFDFA model
```{r}
# Null model without OnTask for MFDFA Width
mfdfa_model_null <- lmer(mfdfa_width ~ (1 | Subject) + (1 | Subject:Session), data = ERD_data_New2)
# ANOVA between the full and null model for MFDFA Width
anova(mfdfa_model_null, mfdfa_model )


```
```{r}
library(dplyr)
library(ggplot2)

# Assuming 'erd_model' and 'mfdfa_model' are already fitted
ERD_data_New2$predicted_ERD <- predict(erd_model, re.form = NA)  # fixed effects only
ERD_data_New2$predicted_mfdfa_width <- predict(mfdfa_model, re.form = NA)

erd_data <- ERD_data_New2 %>%
  select(OnTask, predicted_ERD) %>%
  rename(predicted_value = predicted_ERD) %>%
  mutate(measurement = "ERD")

mfdfa_data <- ERD_data_New2 %>%
  select(OnTask, predicted_mfdfa_width) %>%
  rename(predicted_value = predicted_mfdfa_width) %>%
  mutate(measurement = "MFDFA Width")


```
```{r}

# Plotting MFDFA Width
p_mfdfa <- ggplot(mfdfa_data, aes(x = OnTask, y = predicted_value, fill = OnTask)) +
  geom_boxplot(alpha = 0.5) +
  scale_fill_manual(values = c("on-task" = "blue", "off-task" = "red")) +
  labs(title = "Predicted MFDFA Width by Task Status",
       x = "Task Status",
       y = "Predicted MFDFA Width") +
  theme_minimal() +
  theme(legend.title = element_blank())  # Removes the legend title for clarity

# Print the MFDFA Width plot
print(p_mfdfa)


```
```{r}
# Plotting MFDFA Width
p_mfdfa <- ggplot(mfdfa_data, aes(x = OnTask, y = predicted_value, fill = OnTask)) +
  geom_boxplot(alpha = 0.5) +
  scale_fill_manual(values = c("on-task" = "blue", "off-task" = "red")) +
  labs(title = "Predicted MFDFA Width by Task Status",
       x = "Task Status",
       y = "Predicted MFDFA Width") +
  theme_minimal() +
  theme(legend.title = element_blank())  # Removes the legend title for clarity

# Print the MFDFA Width plot
print(p_mfdfa)



```
```{r}

# Create a violin plot for mfdfa_width by session
p <- ggplot(ERD_data_New2, aes(x = factor(Session), y = mfdfa_width, fill = factor(Session))) +
  geom_violin(trim = FALSE) +
  labs(title = "MFDFA Width by Session",
       x = "Session",
       y = "MFDFA Width") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")  # Use a color palette suitable for categorical data

# Print the plot
print(p)
# Perform the Kruskal-Wallis test
kw_test_result <- kruskal.test(mfdfa_width ~ factor(Session), data = ERD_data_New2)

# Update plot with test results
p + labs(subtitle = paste("Kruskal-Wallis test p-value:", format.pval(kw_test_result$p.value, digits = 3)))

```




#LMM for ERD's over sessions, accounting for participant
```{r}

model <- lmer(ERD ~ factor(Session) + (1|Subject), data = ERD_data_New2)
summary(model)

``` 
#Running post-hoc
```{r}
emm <- emmeans(model, pairwise ~ factor(Session))
summary(emm$contrasts)

``` 


```{r}
# Assuming your data frame is named 'df' and it includes 'ERD', 'Session', and 'Subject'
df_normalized <- ERD_data_New2 %>%
  group_by(Subject) %>%
  mutate(ERD_normalized = (ERD - min(ERD)) / (max(ERD) - min(ERD))) %>%
  ungroup()




# Convert 'Session' to a factor if it's not already
df_normalized$Session <- as.factor(df_normalized$Session)

# Create the boxplot of normalized ERD values
ggplot(df_normalized, aes(x = Session, y = ERD_normalized, fill = Session)) + 
  geom_boxplot() +
  labs(title = "Normalized Boxplots of ERD by Session", 
       x = "Session", 
       y = "Normalized ERD") +
  theme_minimal() +
  theme(legend.position = "none")  # Optional: Remove the legend if not needed





# Assuming your data frame is named 'df' and it includes 'ERD' and 'Session' as columns
# Convert 'Session' to a factor if it's not already
ERD_data_New2$Session <- as.factor(ERD_data_New2$Session)

# Create the boxplot
ggplot(ERD_data_New2, aes(x = Session, y = ERD, fill = Session)) + 
  geom_boxplot() +
  labs(title = "Boxplots of ERD by Session", 
       x = "Session", 
       y = "ERD") +
  theme_minimal() +
  theme(legend.position = "none")  # Remove the legend if not needed

``` 





#LMM for Spectral width over sessions, accounting for participant
```{r}

model_mfdfa <- lmer(mfdfa_width ~ factor(Session) + (1|Subject), data = ERD_data_New2)
summary(model_mfdfa)

``` 
```{r}
emm_mfdfa <- emmeans(model_mfdfa, pairwise ~ factor(Session))
summary(emm_mfdfa$contrasts)

``` 




#LMM for Spectral slopes over sessions, accounting for participant
```{r}

model_slopes <- lmer(slopes ~ factor(Session) + (1|Subject), data = ERD_data_New2)
summary(model_slopes)

``` 
```{r}
emm_slopes <- emmeans(model_slopes, pairwise ~ factor(Session))
summary(emm_slopes$contrasts)

``` 
```{r}
# Assuming your data frame is named 'df' and it includes 'ERD', 'Session', and 'Subject'
df_slopes_normalized <- ERD_data_New2 %>%
  group_by(Subject) %>%
  mutate(slopes_normalized = (slopes - min(slopes)) / (max(slopes) - min(slopes))) %>%
  ungroup()




# Convert 'Session' to a factor if it's not already
df_slopes_normalized$Session <- as.factor(df_slopes_normalized$Session)

# Create the boxplot of normalized ERD values
ggplot(df_slopes_normalized, aes(x = Session, y = slopes_normalized, fill = Session)) + 
  geom_boxplot() +
  labs(title = "Normalized Boxplots of ERD by Session", 
       x = "Session", 
       y = "Normalized ERD") +
  theme_minimal() +
  theme(legend.position = "none")  # Optional: Remove the legend if not needed





# Assuming your data frame is named 'df' and it includes 'ERD' and 'Session' as columns
# Convert 'Session' to a factor if it's not already
ERD_data_New2$Session <- as.factor(ERD_data_New2$Session)

# Create the boxplot
ggplot(ERD_data_New2, aes(x = Session, y = slopes, fill = Session)) + 
  geom_boxplot() +
  labs(title = "Boxplots of slopes by Session", 
       x = "Session", 
       y = "slopes") +
  theme_minimal() +
  theme(legend.position = "none")  # Remove the legend if not needed

``` 

```{r}
#Temp

# Replace "file_path.csv" with the path to your CSV file
ERD_data <- read.csv("E:\\MonsterDataset\\PreprocessedData\\ERD_data.csv")
# Assuming 'data' is your dataframe
ERD_data_New2 <- subset(ERD_data, artifact == 0)


ERD_data_New2_Results <- ERD_data_New2 %>%
  mutate(result = case_when(
    is.na(result) | result == 0 ~ "Poor",
    result == 1 ~ "Good",
    TRUE ~ as.character(result)  # This line handles any other values that might exist
  ))


# View the first few rows of the dataframe
head(ERD_data_New2_Results)


```

#Boxplots for performance (ERD)
```{r}
# Load necessary libraries
#library(ggplot2)
#library(dplyr)

# Remove outliers based on the 1.5*IQR rule

ERD_data_cleaned <- ERD_data_New2_Results %>%
  group_by(`result`) %>%
  mutate(
    Lower_Bound = quantile(ERD, 0.25) - 1.5 * IQR(ERD),
    Upper_Bound = quantile(ERD, 0.75) + 1.5 * IQR(ERD)
  ) %>%
  filter(ERD >= Lower_Bound & ERD <= Upper_Bound) %>%
  select(-Lower_Bound, -Upper_Bound)

ERD_data_cleaned
# Create a violin plot for ERD by performance with outliers removed
p <- ggplot(ERD_data_cleaned, aes(x = `result`, y = ERD, fill = `result`)) +
  geom_violin(trim = FALSE) +
  labs(title = "ERD by Performance (Outliers Removed)",
       x = "Performance",
       y = "ERD") +
  theme_minimal() +
  scale_fill_manual(values = c("Poor" = "blue", "Good" = "red"))

# Print the plot
print(p)


# Perform the Wilcoxon rank-sum test with cleaned data
test_result <- wilcox.test(ERD ~ `result`, data = ERD_data_cleaned)

# Update plot with test results
p + labs(subtitle = paste("Wilcoxon test p-value:", format.pval(test_result$p.value, digits = 3)))

```




#LEGACY Comparing MFDFA full results Task status
```{r}

OntaskMFDFA <- ERD_data_New2["On-task"]
scales <- logscale(scale_min=8, scale_max = 250, scale_ratio = 1.1)

# Initialize sum structures for on-task and off-task, including matrices for log_fq
summed_values_on <- summed_values <- list(x = numeric(length = 1000), order = numeric(length = 1),  q = numeric(length = 21), scales = numeric(length = 36), scale_ratio = numeric(length = 1), log_scale = numeric(length = 36), Hq = numeric(length = 21), Tau = numeric(length = 21),h = numeric(length = 20), Dh = numeric(length = 20) ) # Initialize for each named element
summed_log_fq_on <- matrix(0, nrow = 36, ncol = 21)

summed_values_off <- summed_values <- list(x = numeric(length = 1000), order = numeric(length = 1),  q = numeric(length = 21), scales = numeric(length = 36), scale_ratio = numeric(length = 1), log_scale = numeric(length = 36), Hq = numeric(length = 21), Tau = numeric(length = 21),h = numeric(length = 20), Dh = numeric(length = 20) ) # Initialize for each named element
summed_log_fq_off <- matrix(0, nrow = 36, ncol = 21)




# Counters for on-task and off-task iterations
count_on <- 0
count_off <- 0

# Loop over data
for (i in 1:length(Raw_data)) {
  mf.dfa.trial <- mfdfa(x = Raw_data[[i]], q = c(-5:15), order = 2, scales = scales, scale_ratio = 1.1)
  
  # Determine task status for current iteration
  status <- OntaskMFDFA[[1]][[i]]
  
  if (status == "on-task") {
    count_on <- count_on + 1
    # Sum values for on-task
    for (name in names(mf.dfa.trial)) {
      if (name != "log_fq") {
        summed_values_on[[name]] <- mapply(`+`, summed_values_on[[name]], mf.dfa.trial[[name]], SIMPLIFY = TRUE, USE.NAMES = FALSE)
      }
    }
    summed_log_fq_on <- summed_log_fq_on + mf.dfa.trial$log_fq
  } else { # off-task
    count_off <- count_off + 1
    # Sum values for off-task
    for (name in names(mf.dfa.trial)) {
      if (name != "log_fq") {
        summed_values_off[[name]] <- mapply(`+`, summed_values_off[[name]], mf.dfa.trial[[name]], SIMPLIFY = TRUE, USE.NAMES = FALSE)
      }
    }
    summed_log_fq_off <- summed_log_fq_off + mf.dfa.trial$log_fq
  }
}

# Calculate averages for on-task
averaged_values_on <- lapply(summed_values_on, function(x) x / count_on)
averaged_values_on$log_fq <- summed_log_fq_on / count_on

# Calculate averages for off-task
averaged_values_off <- lapply(summed_values_off, function(x) x / count_off)
averaged_values_off$log_fq <- summed_log_fq_off / count_off





``` 
```{r}
mfdfa.plot(averaged_values_on, do.surrogate = "true")
print(max(averaged_values_on$h) - min(averaged_values_on$h))
```
```{r}
mfdfa.plot(averaged_values_off, do.surrogate = "true")
print(max(averaged_values_off$h) - min(averaged_values_off$h))

```
```{r}
# Function to update the session-specific sums and counters
updateSession <- function(summed_values, summed_log_fq, mf.dfa.trial) {
  for (name in names(mf.dfa.trial)) {
    if (name != "log_fq") {
      summed_values[[name]] <- mapply(`+`, summed_values[[name]], mf.dfa.trial[[name]], SIMPLIFY = TRUE, USE.NAMES = FALSE)
    } else {
      summed_log_fq <- summed_log_fq + mf.dfa.trial$log_fq
    }
  }
  return(list(summed_values = summed_values, summed_log_fq = summed_log_fq))
}


# Average the summed values for each session
averageSession <- function(summed_values, summed_log_fq, count) {
  averaged_values <- lapply(summed_values, function(x) x / count)
  averaged_values$log_fq <- summed_log_fq / count
  return(averaged_values)
}
```
```{r}



Session <- ERD_data_New2["Session"]
# Initialize lists to hold summed values and counters for each session
# Initialize summing structures and counters for each session
summed_values_session1 <- summed_values <- list(x = numeric(length = 1000), order = numeric(length = 1),  q = numeric(length = 21), scales = numeric(length = 36), scale_ratio = numeric(length = 1), log_scale = numeric(length = 36), Hq = numeric(length = 21), Tau = numeric(length = 21),h = numeric(length = 20), Dh = numeric(length = 20) )
summed_log_fq_session1 <- matrix(0, nrow = 36, ncol = 21)
count_session1 <- 0

summed_values_session2 <- summed_values <- list(x = numeric(length = 1000), order = numeric(length = 1),  q = numeric(length = 21), scales = numeric(length = 36), scale_ratio = numeric(length = 1), log_scale = numeric(length = 36), Hq = numeric(length = 21), Tau = numeric(length = 21),h = numeric(length = 20), Dh = numeric(length = 20) )
summed_log_fq_session2 <- matrix(0, nrow = 36, ncol = 21)
count_session2 <- 0

summed_values_session3 <- summed_values <- list(x = numeric(length = 1000), order = numeric(length = 1),  q = numeric(length = 21), scales = numeric(length = 36), scale_ratio = numeric(length = 1), log_scale = numeric(length = 36), Hq = numeric(length = 21), Tau = numeric(length = 21),h = numeric(length = 20), Dh = numeric(length = 20) )
summed_log_fq_session3 <- matrix(0, nrow = 36, ncol = 21)
count_session3 <- 0

summed_values_session4 <- summed_values <- list(x = numeric(length = 1000), order = numeric(length = 1),  q = numeric(length = 21), scales = numeric(length = 36), scale_ratio = numeric(length = 1), log_scale = numeric(length = 36), Hq = numeric(length = 21), Tau = numeric(length = 21),h = numeric(length = 20), Dh = numeric(length = 20) )
summed_log_fq_session4 <- matrix(0, nrow = 36, ncol = 21)
count_session4 <- 0

summed_values_session5 <- summed_values <- list(x = numeric(length = 1000), order = numeric(length = 1),  q = numeric(length = 21), scales = numeric(length = 36), scale_ratio = numeric(length = 1), log_scale = numeric(length = 36), Hq = numeric(length = 21), Tau = numeric(length = 21),h = numeric(length = 20), Dh = numeric(length = 20) )
summed_log_fq_session5 <- matrix(0, nrow = 36, ncol = 21)
count_session5 <- 0


# Define the updateSession function
updateSession <- function(summed_values, summed_log_fq, mf.dfa.trial) {
  for (name in names(mf.dfa.trial)) {
    if (name != "log_fq") {
      summed_values[[name]] <- mapply(`+`, summed_values[[name]], mf.dfa.trial[[name]], SIMPLIFY = TRUE, USE.NAMES = FALSE)
    } else {
      summed_log_fq <- summed_log_fq + mf.dfa.trial$log_fq
    }
  }
  return(list(summed_values = summed_values, summed_log_fq = summed_log_fq))
}

# Define the averageSession function
averageSession <- function(summed_values, summed_log_fq, count) {
  averaged_values <- lapply(summed_values, function(x) x / count)
  averaged_values$log_fq <- summed_log_fq / count
  return(averaged_values)
}




# Loop over your data
for (i in 1:length(Session[[1]])) {
  session_id <- Session[[1]][[i]]
  mf.dfa.trial <- mfdfa(x = Raw_data[[i]], q = c(-5:15), order = 2, scales = scales, scale_ratio = 1.1)
  
  if (session_id == 1) {
    
    updated_session <- updateSession(summed_values_session1, summed_log_fq_session1, mf.dfa.trial)
    summed_values_session1 <- updated_session$summed_values
    summed_log_fq_session1 <- updated_session$summed_log_fq
    count_session1 <- count_session1 + 1
  } 
  
  if (session_id == 2) {
    
    updated_session <- updateSession(summed_values_session2, summed_log_fq_session2, mf.dfa.trial)
    summed_values_session2 <- updated_session$summed_values
    summed_log_fq_session2 <- updated_session$summed_log_fq
    count_session2 <- count_session2 + 1
  } 
  
  if (session_id == 3) {
    
    updated_session <- updateSession(summed_values_session3, summed_log_fq_session3, mf.dfa.trial)
    summed_values_session3 <- updated_session$summed_values
    summed_log_fq_session3 <- updated_session$summed_log_fq
    count_session3 <- count_session3 + 1
  } 
  
  if (session_id == 4) {
    updated_session <- updateSession(summed_values_session4, summed_log_fq_session4, mf.dfa.trial)
    summed_values_session4 <- updated_session$summed_values
    summed_log_fq_session4 <- updated_session$summed_log_fq
    count_session4 <- count_session4 + 1
  }
  
  if (session_id ==5){
    updated_session <- updateSession(summed_values_session5, summed_log_fq_session5, mf.dfa.trial)
    summed_values_session5 <- updated_session$summed_values
    summed_log_fq_session5 <- updated_session$summed_log_fq
    count_session5 <- count_session5 + 1
  }
}


# Calculate averages for each session
averaged_values_session1 <- averageSession(summed_values_session1, summed_log_fq_session1, count_session1)
averaged_values_session2 <- averageSession(summed_values_session2, summed_log_fq_session2, count_session2)
averaged_values_session3 <- averageSession(summed_values_session3, summed_log_fq_session3, count_session3)
averaged_values_session4 <- averageSession(summed_values_session4, summed_log_fq_session4, count_session4)
averaged_values_session5 <- averageSession(summed_values_session5, summed_log_fq_session5, count_session5)

```
```{r}
mfdfa.plot(averaged_values_session1, do.surrogate = "true")
print(max(averaged_values_session1$h) - min(averaged_values_session1$h))

```
```{r}
mfdfa.plot(averaged_values_session2, do.surrogate = "true")
print(max(averaged_values_session2$h) - min(averaged_values_session2$h))

```
```{r}
mfdfa.plot(averaged_values_session3, do.surrogate = "true")
print(max(averaged_values_session3$h) - min(averaged_values_session3$h))

```
```{r}
mfdfa.plot(averaged_values_session4, do.surrogate = "true")
print(max(averaged_values_session4$h) - min(averaged_values_session4$h))

```
```{r}
mfdfa.plot(averaged_values_session5, do.surrogate = "true")
print(max(averaged_values_session5$h) - min(averaged_values_session5$h))

```




```{r}


``` 













####Statistics

```{r}

# Assuming outcomes and sessions are your lists
data <- data.frame(session = unlist(Session), SpectrumWidth = unlist(mf_dfa_widths_trials_list), participant = unlist(ERD_data_New2$Subject))

# Checking for normal distribution in each group
by(data$SpectrumWidth, data$session, function(x) shapiro.test(x)$p.value)



```



```{r}
# If assumptions are met, you can proceed with ANOVA, else use Kruskal-Wallis
# ANOVA
#anova_result <- aov(outcome ~ session, data = data)
#summary(anova_result)

# Kruskal-Wallis
kruskal_result <- kruskal.test(SpectrumWidth ~ session, data = data)
kruskal_result

```

```{r}
# Post-hoc test after Kruskal-Wallis
post_hoc <- pairwise.wilcox.test(data$SpectrumWidth, data$session)

# View the post-hoc test results
print(post_hoc)


```
```{r}
detach("package:reticulate", unload=TRUE)


#require(lme4)
```

```{r}
# Model with participant as a random effect and session as a fixed effect
model <- lmer(SpectrumWidth ~ session + (1|participant), data = data)

# Summary of the model to see the results
summary(model)



```


```{r}
estimate <- 0.0010742
std.error <- 0.0005583
z.value <- estimate / std.error
# Get two-tailed p-value
p.value <- 2 * (1 - pnorm(abs(z.value)))
p.value

```


##THIS IS WHERE SPECTRAL SLOPES BEGIN
```{r}
file_raw_c3 <- "E:\\MonsterDataset\\PreprocessedData\\Baselined_C3_data.csv"
data_c3 <- read.csv(file_raw_c3, header = FALSE)
data_c3 <- as.vector(t(data_c3))
#file_raw_c4 <- "E:\\MonsterDataset\\PreprocessedData\\Baselined_C4_data.csv"
``` 

```{r}

# # Initialize a list to store slopes
# slopes <- list()
# 
# # Process each trial
# for(trial in Raw_data) {
#   # Step 1: Calculate power spectrum
#   ps <- spectrum(trial, plot = FALSE)
# 
#   # Step 2: Transform to log-log space
#   log_freq <- log(ps$freq[ps$freq > 0])  # Avoid log(0)
#   log_spec <- log(ps$spec[ps$freq > 0])
# 
#   # Step 3: Linear regression on log-log transformed data
#   model <- lm(log_spec ~ log_freq)
# 
#   # Step 4: Extract the slope and store it
#   slope <- coef(model)[2]  # the second coefficient is the slope (log_freq)
#   slopes <- c(slopes, slope)
# }

# File path for the slopes data
file_path <- "E:\\MonsterDataset\\PreprocessedData\\slopes_data_C4.csv"

# File to read raw data from
input_file_path <- "E:\\MonsterDataset\\PreprocessedData\\Baselined_C4_data.csv"

# Ensure the file for slopes is empty and set up with the correct headers
write.table(x = data.frame(Slope = numeric(0)), file = file_path, sep = ",", col.names = TRUE, row.names = FALSE)

# Open the input file for reading
con <- file(input_file_path, open = "r")

# Read and process each line one at a time
while (length(line <- readLines(con, n = 1, warn = FALSE)) > 0) {
  # Convert line to numeric data
  trial <- as.numeric(strsplit(line, ",")[[1]])

  # Step 1: Calculate power spectrum
  ps <- spectrum(trial, plot = FALSE)

  # Step 2: Transform to log-log space
  log_freq <- log(ps$freq[ps$freq > 0])  # Avoid log(0)
  log_spec <- log(ps$spec[ps$freq > 0])

  # Step 3: Linear regression on log-log transformed data
  model <- lm(log_spec ~ log_freq)

  # Step 4: Extract the slope and store it
  slope <- coef(model)[2]  # the second coefficient is the slope (log_freq)

  # Append the slope to the file
  write.table(x = data.frame(Slope = slope), file = file_path, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE)
}

# Close the connection
close(con)


```

#Adding slopes to the main dataframe
``` {r}
ERD_data_New2$slopes <- NA

for (i in 1:length(ERD_data_New2$slopes)) {
  ERD_data_New2$slopes[i] <- slopes[[i]]
}
``` 


```{r}
ERD_data_New2

```
```{r}

```

#Over time analysis
```{r}
require(fractalRegression) # (Likens A, Wiltshire T 2023)

# Assuming your data is in a dataframe named `df` and the column is named `column_name`
# Extract the column as a vector
ERD_DFA <- ERD_data_New2$ERD




scalesA <- logscale(scale_min = 8, scale_max = length(ERD_DFA)/4, scale_ratio = 1.5)

dfa_result_ERD <- dfa(x = ERD_DFA, order = 2, verbose = 1, scales=scalesA, scale_ratio = 1.5)
dfa_result_ERD

```


```{r}
# Assuming your dataframe is loaded into a variable named `df`

# Load necessary libraries
library(dplyr)

# Remove 'Channel' and 'OnTask' columns
#ERD_data_New2 <- ERD_data_New2 %>% select(-Channel, -OnTask)

# Group by every two rows using the row number
ERD_data_New2 <- ERD_data_New2 %>%
  mutate(group_id = (row_number() - 1) %/% 2)


# Summarize each group by calculating the mean of numerical columns
Slim_data_thesis <- ERD_data_New2 %>%
  group_by( Subject, Session, Trial,group_id) %>%
  summarize(across(where(is.numeric), mean), .groups = "drop")

# Remove the temporary 'group_id' column
Slim_data_thesis$group_id <- NULL



Slim_data_thesis
```

```{r}
library(fractalRegression)
library(ggplot2)
library(dplyr)

# Assuming your data is loaded into a dataframe named ERD_data_New2
# And it contains 'participant_number', 'session_number', 'ERD', and 'forcedresult' columns

# Create a unique identifier for each participant-session combination
Slim_data_thesis$participant_session <- interaction(Slim_data_thesis$Subject, Slim_data_thesis$Session)

# Split data by this unique identifier
session_list <- split(Slim_data_thesis, Slim_data_thesis$participant_session)

# Function to perform DFA and calculate average forced result
perform_analysis <- function(data) {
  scalesA <- logscale(scale_min = 8, scale_max = length(data$ERD)/4, scale_ratio = 1.5)
  dfa_result <- dfa(x = data$ERD, order = 2, verbose = 1, scales = scalesA, scale_ratio = 1.5)
  avg_forcedresult <- mean(data$forcedresult, na.rm = TRUE)
  
  # Assuming the DFA result object has a scaling exponent we can extract
  alpha <- dfa_result$alpha  # Modify according to the actual structure of the DFA output
  
  return(c(alpha, avg_forcedresult))
}

# Apply the analysis function to each participant-session and rbind results into a dataframe
results <- do.call(rbind, lapply(session_list, perform_analysis))
colnames(results) <- c("DFA_Exponent", "AverageForcedResult")

# Convert the results into a dataframe and add participant-session as a column
results_df <- as.data.frame(results)
results_df$Participant_Session <- rownames(results_df)

# Create a correlation plot
ggplot(results_df, aes(x = AverageForcedResult, y = DFA_Exponent)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(title = "Correlation between DFA Exponent and Average Forced Result",
       x = "DFA Exponent",
       y = "Average Forced Result")

# Print the correlation coefficient
correlation <- cor(results_df$DFA_Exponent, results_df$AverageForcedResult, use = "complete.obs")
print(paste("Correlation coefficient:", correlation))

```

```{r}
library(fractalRegression)
library(ggplot2)
library(dplyr)

# Assuming your data is loaded into a dataframe named ERD_data_New2
# And it contains 'participant_number', 'session_number', 'ERD', and 'forcedresult' columns

# Create a unique identifier for each participant-session combination
ERD_data_New2$participant_session <- interaction(ERD_data_New2$Subject, ERD_data_New2$Session)

# Split data by this unique identifier
session_list <- split(ERD_data_New2, ERD_data_New2$participant_session)

# Function to perform DFA and calculate average forced result
perform_analysis <- function(data) {
  scalesA <- logscale(scale_min = 8, scale_max = length(data$mf_dfa_widths)/4, scale_ratio = 1.5)
  dfa_result <- dfa(x = data$mf_dfa_widths, order = 2, verbose = 1, scales = scalesA, scale_ratio = 1.5)
  avg_forcedresult <- mean(data$forcedresult, na.rm = TRUE)
  
  # Assuming the DFA result object has a scaling exponent we can extract
  alpha <- dfa_result$alpha  # Modify according to the actual structure of the DFA output
  
  return(c(alpha, avg_forcedresult))
}

# Apply the analysis function to each participant-session and rbind results into a dataframe
results <- do.call(rbind, lapply(session_list, perform_analysis))
colnames(results) <- c("DFA_Exponent", "AverageForcedResult")

# Convert the results into a dataframe and add participant-session as a column
results_df <- as.data.frame(results)
results_df$Participant_Session <- rownames(results_df)

# Create a correlation plot
ggplot(results_df, aes(x = DFA_Exponent, y = AverageForcedResult)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(title = "Correlation between DFA Exponent and Average Forced Result",
       x = "DFA Exponent",
       y = "Average Forced Result")

# Print the correlation coefficient
correlation <- cor(results_df$DFA_Exponent, results_df$AverageForcedResult, use = "complete.obs")
print(paste("Correlation coefficient:", correlation))

```


```{r}
library(fractalRegression)
library(ggplot2)
library(dplyr)

# Assuming your data is loaded into a dataframe named Slim_data_thesis
# And it contains 'participant_number', 'session_number', 'ERD', and 'forcedresult' columns

# Create a unique identifier for each participant-session combination
Slim_data_thesis$participant_session <- interaction(Slim_data_thesis$Subject, Slim_data_thesis$Session)

# Split data by this unique identifier
session_list <- split(Slim_data_thesis, Slim_data_thesis$participant_session)

# Function to perform DFA and calculate average forced result
perform_analysis <- function(data) {
  scalesA <- logscale(scale_min = 8, scale_max = length(data$combined_slopes_C3_C4)/4, scale_ratio = 1.5)
  dfa_result <- dfa(x = data$combined_slopes_C3_C4, order = 2, verbose = 1, scales = scalesA, scale_ratio = 1.5)
  avg_forcedresult <- mean(data$forcedresult, na.rm = TRUE)
  
  # Assuming the DFA result object has a scaling exponent we can extract
  alpha <- dfa_result$alpha  # Modify according to the actual structure of the DFA output
  
  return(c(alpha, avg_forcedresult))
}

# Apply the analysis function to each participant-session and rbind results into a dataframe
results <- do.call(rbind, lapply(session_list, perform_analysis))
colnames(results) <- c("DFA_Exponent", "AverageForcedResult")

# Convert the results into a dataframe and add participant-session as a column
results_df <- as.data.frame(results)
results_df$Participant_Session <- rownames(results_df)

# Create a correlation plot
ggplot(results_df, aes(x = DFA_Exponent, y = AverageForcedResult)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(title = "Correlation between DFA Exponent and Average Forced Result",
       x = "DFA Exponent",
       y = "Average Forced Result")

# Print the correlation coefficient
correlation <- cor(results_df$DFA_Exponent, results_df$AverageForcedResult, use = "complete.obs")
print(paste("Correlation coefficient:", correlation))

```




```{r}
library(fractalRegression)
library(ggplot2)
library(dplyr)
library(forecast)  # for acf function and more

# Function to calculate ACF and find the first zero crossing
calculate_acf_and_zero_crossing <- function(data) {
  acf_values <- acf(data$combined_slopes_C3_C4, plot = FALSE)$acf
  # Finding the first zero crossing
  zero_crossings <- which(diff(sign(acf_values)) != 0)
  first_zero_crossing <- min(zero_crossings, na.rm = TRUE)
  return(list(acf_values = acf_values, first_zero_crossing = first_zero_crossing))
}

# Function to perform analysis and extract ACF and average forced result
perform_acf_analysis <- function(data) {
  acf_result <- calculate_acf_and_zero_crossing(data)
  avg_forcedresult <- mean(data$forcedresult, na.rm = TRUE)
  
  return(c(FirstZeroCrossing = acf_result$first_zero_crossing, AverageForcedResult = avg_forcedresult, ACF = I(acf_result$acf_values)))
}

# Apply the analysis function to each participant-session
results_acf <- do.call(rbind, lapply(session_list, perform_acf_analysis))
colnames(results_acf) <- c("FirstZeroCrossing", "AverageForcedResult", "ACF")

# Convert the results into a dataframe and add participant-session as a column
results_acf_df <- as.data.frame(results_acf)
results_acf_df$Participant_Session <- rownames(results_acf_df)

# Correlation plot for first zero crossing and average forced result
ggplot(results_acf_df, aes(x = FirstZeroCrossing, y = AverageForcedResult)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(title = "Correlation between First Zero Crossing of ACF and Average Forced Result",
       x = "First Zero Crossing of ACF",
       y = "Average Forced Result")

# Print the correlation coefficient for zero crossing
correlation_zero_crossing <- cor(results_acf_df$FirstZeroCrossing, results_acf_df$AverageForcedResult, use = "complete.obs")
print(paste("Correlation coefficient (Zero Crossing):", correlation_zero_crossing))

# Averaging ACF across all participants for each session
average_acfs <- tapply(results_acf_df$ACF, results_acf_df$Participant_Session, function(x) mean(unlist(x), na.rm = TRUE))
# Plot average ACF
average_acf_plot <- ts(average_acfs)  # Assuming equal spacing and starting at 1
autoplot(average_acf_plot) + labs(title = "Average Autocorrelation Function per Session",
                                  x = "Lag",
                                  y = "Average Autocorrelation")


```
```{r}
library(forecast)  # For the ACF function
library(ggplot2)
library(dplyr)

# Assuming your data is loaded into a dataframe named Slim_data_thesis
# And it contains 'participant_number', 'session_number', 'ERD', and 'forcedresult' columns

# Create a unique identifier for each participant
Slim_data_thesis$participant <- as.factor(Slim_data_thesis$Subject)

# Group data by participant
participant_groups <- split(Slim_data_thesis, Slim_data_thesis$participant)

# Function to calculate ACF for a participant's combined trials
calculate_participant_acf <- function(data) {
  # Combine all ERD values for the participant across sessions
  combined_erd <- unlist(data$combined_slopes_C3_C4)
  
  # Calculate ACF
  acf_result <- acf(combined_erd,lag.max = 315,  plot = FALSE)
  
  # Store ACF values
  acf_values <- acf_result$acf
  return(list(ACF = acf_values))
}

# Apply the function to each participant group
results_acf_participants <- lapply(participant_groups, calculate_participant_acf)

# Optionally, you might want to plot the ACF for each participant or analyze further
# Example: Plotting ACF for a single participant (e.g., the first participant)
acf_values_example <- results_acf_participants[[1]]$ACF
acf_plot <- ts(acf_values_example)
autoplot(acf_plot) + 
  labs(title = "Autocorrelation Function for Participant 1",
       x = "Lag",
       y = "Autocorrelation")

# To display plots for all participants (in separate plots)
pdf("All_Participants_ACF_Plots.pdf")
for (i in seq_along(results_acf_participants)) {
  plot(acf(ts(unlist(results_acf_participants[[i]]$ACF))), main = paste("ACF for Participant", names(results_acf_participants)[i]))
}
dev.off()



```


```{r}
library(forecast)  # For additional time series tools
library(ggplot2)
library(dplyr)

# Function to calculate ACF for a participant's combined trials
calculate_participant_acf <- function(data, max_lag) {
  # Combine all ERD values for the participant across sessions
  combined_erd <- unlist(data$combined_slopes_C3_C4)
  
  # Calculate ACF with specified max lag and ensure non-NA data
  if (any(!is.na(combined_erd))) {
    acf_result <- acf(combined_erd, lag.max = max_lag, plot = FALSE)
    # Store ACF values
    acf_values <- acf_result$acf
  } else {
    acf_values <- rep(NA, max_lag + 1)  # +1 because acf includes lag 0
  }
  return(list(ACF = acf_values))
}

# Set max_lag to n/10 for a total of 3150 samples
max_lag <- 315

# Apply the function to each participant group with the specified max lag
results_acf_participants <- lapply(participant_groups, calculate_participant_acf, max_lag = max_lag)

# Extract a single participant's ACF values for plotting
acf_values_example <- unlist(results_acf_participants[[1]]$ACF)

# Ensure the length of the ACF values matches expected lags + 1 (for lag zero)
print(length(acf_values_example))  # Debug: Check the actual length of ACF data

# Plot the ACF using base R to ensure all lags are shown
plot(acf_values_example, type = "h", main = "ACF Plot with Increased Lags", xlab = "Lag", ylab = "Autocorrelation")




```









#R Get data for Logitudinal
```{r}


Longitudinal1 <- ERD_data_New2 %>%
  mutate(Trials = as.numeric(Trials)) %>%
  group_by(Subject, Session, Trials, forcedresult) %>%
  summarize(ERD_avg = mean(ERD))

Longitudinal2 <- ERD_data_New2 %>%
  mutate(Trials = as.numeric(Trials)) %>%
  group_by(Subject, Session, Trials, forcedresult) %>%
  summarize(mfdfa_avg = mean(mf_dfa_widths)) 

Longitudinal3 <- ERD_data_New2 %>%
  mutate(Trials = as.numeric(Trials)) %>%
  group_by(Subject, Session, Trials, forcedresult) %>%
  summarize(mfdfa_rest_avg = mean(mf_dfa_rest_widths))
  
Longitudinal <- merge(Longitudinal1, Longitudinal2)
Longitudinal <- merge(Longitudinal, Longitudinal3)

# Sort the merged dataset by Subject, Session, and Trials
Longitudinal <- Longitudinal[order(Longitudinal$Subject, Longitudinal$Session, Longitudinal$Trials), ]

Longitudinal["mfdfa_dif_avg"] <- (Longitudinal$mfdfa_avg - Longitudinal$mfdfa_rest_avg)

Longitudinal

summary(Longitudinal$ERD_avg)
summary(Longitudinal$mfdfa_avg)
summary(Longitudinal$mfdfa_rest_avg)
summary(Longitudinal$mfdfa_dif_avg)

```

#R Calculate DFA
```{r}
perform_dfa <- function(data) {
  if(length(data) == 0) {
    warning("No data received by perform_dfa")
    return(NA)  # Returning NA or another placeholder if no data is available
  }
  dfa_result <- dfa(data, order = 2, verbose = 1, scales = logscale(16, 225/4, 1.1), scale_ratio = 1.1)
  #print(dfa_result$alpha)
  return(dfa_result$alpha)  # return the scaling exponent from the DFA result
}

dfa_ERD_avg<- data.frame()
# Apply DFA to each subject and session for ERD_avg and mfdfa_avg
longitudinal_clean$dfa_ERD_avg <- with(longitudinal_clean, ave(ERD_avg, Subject, Session, FUN = perform_dfa))
longitudinal_clean$dfa_mfdfa_avg <- with(longitudinal_clean, ave(mfdfa_avg, Subject, Session, FUN = perform_dfa))
longitudinal_clean$dfa_mfdfa_dif_avg <- with(longitudinal_clean, ave(mfdfa_dif_avg, Subject, Session, FUN = perform_dfa))


unique_dfa_values <- longitudinal_clean %>%
  group_by(Subject, Session) %>%
  summarise(dfa_ERD = first(dfa_ERD_avg),
            dfa_mfdfa = first(dfa_mfdfa_avg),
            dfa_mfdfa_dif = first(dfa_mfdfa_dif_avg),
            .groups = 'drop')  # 'drop' option to prevent the resulting tibble from being grouped

# View the unique DFA values
summary(unique_dfa_values$dfa_ERD)
summary(unique_dfa_values$dfa_mfdfa)
summary(unique_dfa_values$dfa_mfdfa_dif)


```
#R Get residuals to determine detrending order
```{r}
# Define the participant and session you are interested in
participant_number <- 7  # Replace "ParticipantID" with the actual ID
session_number <- 1  # Replace "SessionID" with the actual session number

selected_data <- longitudinal_clean %>%
  filter(Subject == participant_number, Session == session_number)

temp_dfa_result <- dfa(selected_data$ERD_avg , order = 1, verbose = 1, scales = logscale(4, 225/4, 1.1), scale_ratio = 1.1)
# Creating a dataframe from your DFA results
dfa_data <- data.frame(
  log_scales = temp_dfa_result$log_scales,
  log_rms = temp_dfa_result$log_rms
)



# Creating the plot
plot <- ggplot(dfa_data, aes(x = log_scales, y = log_rms)) + 
  geom_point() +  # Add points
  geom_smooth(method = "lm", color = "blue") +  # Add a linear regression line
  labs(title = "Log RMS vs. Log Scales",
       x = "Log of Scales",
       y = "Log of RMS Fluctuations") +
  theme_minimal()  # Use a minimal theme for a clean plot

# Display the plot
print(plot)


temp_dfa_result <- dfa(selected_data$ERD_avg , order = 2, verbose = 1, scales = logscale(4, 225/4, 1.1), scale_ratio = 1.1)
# Creating a dataframe from your DFA results
dfa_data <- data.frame(
  log_scales = temp_dfa_result$log_scales,
  log_rms = temp_dfa_result$log_rms
)



# Creating the plot
plot <- ggplot(dfa_data, aes(x = log_scales, y = log_rms)) + 
  geom_point() +  # Add points
  geom_smooth(method = "lm", color = "blue") +  # Add a linear regression line
  labs(title = "Log RMS vs. Log Scales",
       x = "Log of Scales",
       y = "Log of RMS Fluctuations") +
  theme_minimal()  # Use a minimal theme for a clean plot

# Display the plot
print(plot)

```

#R LMM's for DFA values
```{r}

#na_indices = which(is.na(Longitudinal$dfa_mfdfa_avg))
#Longitudinal<- Longitudinal[-na_indices, ]

performance <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(accuracy = mean(forcedresult))

summed_mfdfa <- ERD_data_New2 %>%
  group_by(Subject,Session) %>%
  summarise(mfdfavalues = mean(mf_dfa_widths))# 

summed_mfdfa_dif <- ERD_data_New2 %>%
  group_by(Subject, Session) %>%
  summarise(mfdfa_dif_values = (mean(mf_dfa_widths)- mean(mf_dfa_rest_widths)))# 

summed_ERD <- ERD_data_New2 %>%
  group_by(Subject,Session) %>%
  summarise(ERDvalues = mean(ERD))# 

performance_mfdfa <- merge(summed_mfdfa, performance)
performance_mfdfa <- merge(performance_mfdfa, summed_mfdfa_dif)
performance_mfdfa <- merge(performance_mfdfa, summed_ERD)

unique_dfa_values <- unique_dfa_values %>%
  left_join(performance_mfdfa %>% select(Subject, Session, accuracy), by = c("Subject", "Session"))


model_ERD <- lmer(dfa_ERD ~ accuracy + Session + (1|Subject), data = unique_dfa_values)
model_MFDFA <- lmer(dfa_mfdfa ~ accuracy + Session + (1|Subject), data = unique_dfa_values)
model_MFDFA_dif <- lmer(dfa_mfdfa_dif ~ accuracy + Session + (1|Subject), data = unique_dfa_values)

summary(model_ERD)
summary(model_MFDFA)
summary(model_MFDFA_dif)


#anova(model_ERD, model_MFDFA, model_MFDFA_dif)

```
#R DFA over sessions plotted
```{r}
# Load necessary library
library(ggplot2)
library(ggpubr)
# Combine both DFA results for plotting
#longitudinal_clean["Type"] <- 'ERD'
#longitudinal_clean["dfa_Value"] <- longitudinal_clean$dfa_ERD_avg
temp <- longitudinal_clean
temp["Type"] <- 'MFDFA'
temp["dfa_Value"] <- temp$dfa_mfdfa_avg

temp1 <- longitudinal_clean
temp1["Type"] <- 'ERD'
temp1["dfa_Value"] <- temp$dfa_ERD_avg


#Longitudinal_plot <- rbind(longitudinal_clean, temp)

temp2 <- longitudinal_clean
temp2["Type"] <- 'MFDFA_dif'
temp2["dfa_Value"] <- temp2$dfa_mfdfa_dif_avg

Longitudinal_plot <- rbind(temp, temp1)
Longitudinal_plot <- rbind(Longitudinal_plot, temp2)

# Calculate mean DFA values per session and type
library(dplyr)

DFA_values <- Longitudinal_plot %>%
  group_by(Subject, Session, Type) %>%
  summarize(dfa_Value = first(dfa_Value), accuracy = mean(forcedresult))




kmeans_result <- kmeans(DFA_values$accuracy, centers = 2)

# Add cluster results to the dataframe
DFA_values$cluster <- kmeans_result$cluster

# To ensure consistent labeling, you might want to relabel the clusters based on mean accuracy
cluster_labels <- aggregate(accuracy ~ cluster, DFA_values, mean)
names(cluster_labels) <- c('cluster', 'mean_accuracy')
cluster_labels <- cluster_labels[order(-cluster_labels$mean_accuracy),]
DFA_values$cluster <- factor(DFA_values$cluster, levels = cluster_labels$cluster, labels = c('High', 'Low'))

DFA_values["cluster"] <- DFA_values$cluster
DFA_values$cluster <- as.factor(DFA_values$cluster)


mean_dfa <- DFA_values %>%
  group_by(Session, Type, cluster) %>%
  summarize(mean_dfa = mean(dfa_Value, na.rm = TRUE),
            sd_dfa = sd(dfa_Value, na.rm = TRUE),
            se_dfa = sd_dfa / sqrt(n()), .groups = 'drop')


ggplot(mean_dfa, aes(x = Session, y = mean_dfa, color = cluster, linetype = cluster)) +
  geom_line() +
  geom_errorbar(aes(ymin = mean_dfa - se_dfa, ymax = mean_dfa + se_dfa), width = 0.2) +
  facet_wrap(~Type) +
  labs(title = "Mean DFA Values Across Sessions by Cluster", 
       x = "Session", y = "DFA Scaling Exponent") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = 1:7) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +
  geom_hline(yintercept = 0.5, color = "black", linetype = "dashed")
```

```{r}

subset_data <- Longitudinal[Longitudinal$Session == 3 & Longitudinal$Subject == 1,]



acf_result <- acf(subset_data$ERD_avg, plot = TRUE)
acf_values <- acf_result$acf  # Extract ACF values

# Plot the ACF results
acf_data_frame <- data.frame(Lag = seq_along(acf_values) - 1, ACF = acf_values)
N <- length(subset_data$ERD_avg)
confidence_limit <- 2 / sqrt(N)

# Plot the ACF results with significance lines
acf_data_frame <- data.frame(Lag = seq_along(acf_values) - 1, ACF = acf_values)
ggplot(acf_data_frame, aes(x = Lag, y = ACF)) +
  geom_line() +
  geom_hline(yintercept = confidence_limit, color = "red", linetype = "dashed") +
  geom_hline(yintercept = -confidence_limit, color = "red", linetype = "dashed") +
  labs(title = "Autocorrelation Function for ERD_avg of Session 1, Participant 1", 
       x = "Lag", y = "ACF Value") +
  theme_minimal()
```


```{r}
library(ggplot2)

# Filter data for Participant 1 across all sessions
subset_data <- Longitudinal[Longitudinal$Subject == "1",]

# Check and create a list to store ACF results
acf_results <- list()

# Loop through each session available for the participant
unique_sessions <- unique(subset_data$Session)
for (session in unique_sessions) {
  session_data <- subset_data[subset_data$Session == session,]

  # Calculate ACF for ERD_avg for the current session
  if (length(session_data$ERD_avg) > 10) {  # Check if sufficient data points
    acf_result <- acf(session_data$ERD_avg, plot = FALSE)
    acf_results[[as.character(session)]] <- acf_result$acf
  } else {
    acf_results[[as.character(session)]] <- rep(NA, 21)  # Assuming default lag of 20
  }
}

# Convert the list of ACF results to a data frame for plotting
acf_data_frame <- do.call(rbind, lapply(names(acf_results), function(x) {
  data.frame(Session = x, Lag = 0:23, ACF = acf_results[[x]])
}))

# Plot the ACF results using ggplot2
ggplot(acf_data_frame, aes(x = Lag, y = ACF, color = Session)) +
  geom_line() +
  labs(title = "Autocorrelation Function for ERD_avg across Sessions",
       x = "Lag", y = "ACF Value",
       color = "Session") +
  theme_minimal()


```


#ACF over sessions
```{r}
library(dplyr)
library(ggplot2)

# Create a list to store ACF results for each session and participant
acf_results <- list()

# Loop over each participant
unique_participants <- unique(Longitudinal$Subject)
for (participant in unique_participants) {
    # Loop through each session for the current participant
    participant_data <- Longitudinal[Longitudinal$Subject == participant,]
    unique_sessions <- unique(participant_data$Session)
    for (session in unique_sessions) {
        session_data <- participant_data[participant_data$Session == session,]
    
        # Calculate ACF for ERD_avg for the current session and participant
        acf_key <- paste(participant, session, sep = "_")
        if (length(session_data$ERD_avg) > 10) {  # Check if sufficient data points
            acf_result <- acf(session_data$ERD_avg, plot = FALSE)
            acf_values <- acf_result$acf
            lag_length <- min(length(acf_values), 21)  # Limit to 20 lags
            acf_results[[acf_key]] <- data.frame(Lag = 0:(lag_length - 1), ACF = acf_values[1:lag_length], N = length(session_data$ERD_avg))
        } else {
            acf_results[[acf_key]] <- data.frame(Lag = 0:20, ACF = rep(NA, 21), N = length(session_data$ERD_avg))  # Assuming default lag of 20
        }
    }
}

# Combine all ACF results into one data frame
acf_data_frame <- do.call(rbind, lapply(names(acf_results), function(x) {
    parts <- strsplit(x, "_")[[1]]
    participant <- parts[1]
    session <- parts[2]
    cbind(Participant = participant, Session = session, acf_results[[x]])
}))

# Calculate average ACF per session across all participants
average_acf_data <- acf_data_frame %>%
    group_by(Session, Lag) %>%
    summarize(mean_acf = mean(ACF, na.rm = TRUE),
              se_acf = sd(ACF, na.rm = TRUE) / sqrt(n()),
              .groups = 'drop')

# Filter out Lag 0
average_acf_data <- average_acf_data %>%
    filter(Lag > 0)

# Calculate the confidence bounds for significance
#N <- nrow(Longitudinal$ERD_avg)  # Assuming each session has the same number of observations
confidence_limit <- 2 / sqrt(96972)

# Plot the average ACF results using ggplot2
ggplot(average_acf_data, aes(x = Lag, y = mean_acf, color = as.factor(Session))) +
    geom_line() +
    geom_hline(yintercept = confidence_limit, linetype = "dashed", color = "red") +
    geom_hline(yintercept = -confidence_limit, linetype = "dashed", color = "red") +
    labs(title = "Average Autocorrelation Function for ERD_avg across Sessions and Participants",
         x = "Lag", y = "Average ACF Value",
         color = "Session") +
    theme_minimal() +
    scale_x_continuous(breaks = pretty(average_acf_data$Lag, n = 20))

```
```{r}
library(ggplot2)

# Create a list to store ACF results for each session and participant
acf_results <- list()

# Loop over each participant
unique_participants <- unique(Longitudinal$Subject)
for (participant in unique_participants) {
    # Loop through each session for the current participant
    participant_data <- Longitudinal[Longitudinal$Subject == participant,]
    unique_sessions <- unique(participant_data$Session)
    for (session in unique_sessions) {
        session_data <- participant_data[participant_data$Session == session,]

        # Calculate ACF for mfdfa_avg for the current session and participant
        acf_key <- paste(participant, session, sep = "_")
        if (length(session_data$mfdfa_avg) > 10) {  # Check if sufficient data points
            acf_result <- acf(session_data$mfdfa_avg, plot = FALSE)
            acf_values <- acf_result$acf
            lag_length <- min(length(acf_values), 21)  # Limit to 20 lags
            acf_results[[acf_key]] <- data.frame(Lag = 0:(lag_length - 1), ACF = acf_values[1:lag_length])
        } else {
            acf_results[[acf_key]] <- data.frame(Lag = 0:20, ACF = rep(NA, 21))  # Assuming default lag of 20
        }
    }
}

# Combine all ACF results into one data frame
acf_data_frame <- do.call(rbind, lapply(names(acf_results), function(x) {
    parts <- strsplit(x, "_")[[1]]
    participant <- parts[1]
    session <- parts[2]
    cbind(Participant = participant, Session = session, acf_results[[x]])
}))

# Calculate average ACF per session across all participants
average_acf_data <- aggregate(ACF ~ Session + Lag, data = acf_data_frame, mean, na.rm = TRUE)

average_acf_data <- average_acf_data %>%
    filter(Lag > 0)

# Calculate the confidence bounds for significance
#N <- nrow(Longitudinal$ERD_avg)  # Assuming each session has the same number of observations
confidence_limit <- 2 / sqrt(96972)



# Plot the average ACF results using ggplot2
ggplot(average_acf_data, aes(x = Lag, y = ACF, color = Session)) +
    geom_line() +
    geom_hline(yintercept = confidence_limit, linetype = "dashed", color = "red") +
    geom_hline(yintercept = -confidence_limit, linetype = "dashed", color = "red") +
    labs(title = "Average Autocorrelation Function for mfdfa_avg across Sessions and Participants",
         x = "Lag", y = "Average ACF Value",
         color = "Session") +
    theme_minimal()+
    scale_x_continuous(breaks = pretty(average_acf_data$Lag, n = 20))




```
```{r}
library(ggplot2)


#


# Create a list to store ACF results for each session and participant
acf_results <- list()

# Loop over each participant
unique_participants <- unique(Longitudinal$Subject)
for (participant in unique_participants) {
    # Loop through each session for the current participant
    participant_data <- Longitudinal[Longitudinal$Subject == participant,]
    unique_sessions <- unique(participant_data$Session)
    for (session in unique_sessions) {
        session_data <- participant_data[participant_data$Session == session,]

        # Calculate ACF for mfdfa_dif_avg for the current session and participant
        acf_key <- paste(participant, session, sep = "_")
        if (length(session_data$mfdfa_dif_avg) > 10) {  # Check if sufficient data points
            acf_result <- acf(session_data$mfdfa_dif_avg, plot = FALSE)
            acf_values <- acf_result$acf
            lag_length <- min(length(acf_values), 21)  # Limit to 20 lags
            acf_results[[acf_key]] <- data.frame(Lag = 0:(lag_length - 1), ACF = acf_values[1:lag_length])
        } else {
            acf_results[[acf_key]] <- data.frame(Lag = 0:20, ACF = rep(NA, 21))  # Assuming default lag of 20
        }
    }
}

# Combine all ACF results into one data frame
acf_data_frame <- do.call(rbind, lapply(names(acf_results), function(x) {
    parts <- strsplit(x, "_")[[1]]
    participant <- parts[1]
    session <- parts[2]
    cbind(Participant = participant, Session = session, acf_results[[x]])
}))

# Calculate average ACF per session across all participants
average_acf_data <- aggregate(ACF ~ Session + Lag, data = acf_data_frame, mean, na.rm = TRUE)
average_acf_data <- average_acf_data %>%
    filter(Lag > 0)

# Calculate the confidence bounds for significance
#N <- nrow(Longitudinal$ERD_avg)  # Assuming each session has the same number of observations
confidence_limit <- 2 / sqrt(96972)



# Plot the average ACF results using ggplot2
ggplot(average_acf_data, aes(x = Lag, y = ACF, color = Session)) +
    geom_line() +
    geom_hline(yintercept = confidence_limit, linetype = "dashed", color = "red") +
    geom_hline(yintercept = -confidence_limit, linetype = "dashed", color = "red") +
    labs(title = "Average Autocorrelation Function for mfdfa_avg across Sessions and Participants",
         x = "Lag", y = "Average ACF Value",
         color = "Session") +
    theme_minimal()+
    scale_x_continuous(breaks = pretty(average_acf_data$Lag, n = 20))


```
#Final ACF
```{r}
library(dplyr)
library(ggplot2)
library(gridExtra)

# Function to calculate ACF and average ACF for a given metric
calculate_acf <- function(data, metric) {
  acf_results <- list()
  
  # Loop over each participant
  unique_participants <- unique(data$Subject)
  for (participant in unique_participants) {
      # Loop through each session for the current participant
      participant_data <- data[data$Subject == participant,]
      unique_sessions <- unique(participant_data$Session)
      for (session in unique_sessions) {
          session_data <- participant_data[participant_data$Session == session,]
          
          # Calculate ACF for the specified metric for the current session and participant
          acf_key <- paste(participant, session, sep = "_")
          if (length(session_data[[metric]]) > 10) {  # Check if sufficient data points
              acf_result <- acf(session_data[[metric]], plot = FALSE)
              acf_values <- acf_result$acf
              lag_length <- min(length(acf_values), 21)  # Limit to 20 lags
              acf_results[[acf_key]] <- data.frame(Lag = 0:(lag_length - 1), ACF = acf_values[1:lag_length], N = length(session_data[[metric]]))
          } else {
              acf_results[[acf_key]] <- data.frame(Lag = 0:20, ACF = rep(NA, 21), N = length(session_data[[metric]]))  # Assuming default lag of 20
          }
      }
  }

  # Combine all ACF results into one data frame
  acf_data_frame <- do.call(rbind, lapply(names(acf_results), function(x) {
      parts <- strsplit(x, "_")[[1]]
      participant <- parts[1]
      session <- parts[2]
      cbind(Participant = participant, Session = session, acf_results[[x]])
  }))

  # Calculate average ACF per session across all participants
  average_acf_data <- acf_data_frame %>%
      group_by(Session, Lag) %>%
      summarize(mean_acf = mean(ACF, na.rm = TRUE),
                se_acf = sd(ACF, na.rm = TRUE) / sqrt(n()),
                .groups = 'drop')

  # Filter out Lag 0
  average_acf_data <- average_acf_data %>%
      filter(Lag > 0)

  # Return the average ACF data
  return(average_acf_data)
}

# Calculate ACF for ERD_avg
average_acf_data_erd <- calculate_acf(Longitudinal, "ERD_avg")

# Calculate ACF for mfdfa_avg
average_acf_data_mfdfa <- calculate_acf(Longitudinal, "mfdfa_avg")

# Calculate ACF for mfdfa_dif_avg
average_acf_data_mfdfa_dif <- calculate_acf(Longitudinal, "mfdfa_dif_avg")

# Calculate the confidence bounds for significance
N <- 96972  # Assuming a fixed number of observations for the confidence bound calculation
confidence_limit <- 2 / sqrt(N)

library(ggplot2)
library(gridExtra)

# Assuming `average_acf_data_erd`, `average_acf_data_mfdfa`, and `average_acf_data_mfdfa_dif` are already calculated
average_acf_data_erd["Type"] <- "ERD"
average_acf_data_mfdfa["Type"] <- "MFDFA"
average_acf_data_mfdfa_dif["Type"] <- "MFDFA_DIF"

completeACF <- rbind(average_acf_data_erd,average_acf_data_mfdfa,average_acf_data_mfdfa_dif )
# Plot the average ACF results for ERD_avg
ggplot(completeACF, aes(x = Lag, y = mean_acf, color = as.factor(Session))) +
    geom_line() +
    geom_hline(yintercept = confidence_limit, linetype = "dashed", color = "red") +
    geom_hline(yintercept = -confidence_limit, linetype = "dashed", color = "red") +
    facet_wrap(~Type) +
    labs(title = "Autocorrelation plots of metrics describing MI",
         x = "Lag", y = "Average ACF Value",
         color = "Session") +
    theme_minimal() +
    theme(legend.position = "right") +
    scale_x_continuous(breaks = pretty(average_acf_data_erd$Lag, n = 10)) +
    theme(aspect.ratio = 1)  # Adjust the aspect ratio as needed

ggsave("ACF.png", width = 16, height = 8, units = "in")
``` 
```{r}
library(Hmisc)
describe(Longitudinal$ERD_avg)



```


#New time analysis
```{r}
Longitudinal
performance_mfdfa["LI"] <-performance$LI

```
```{r}
remove_outliers <- function(df, columns) {
  for (col in columns) {
    Q1 <- quantile(df[[col]], 0.25, na.rm = TRUE)
    Q3 <- quantile(df[[col]], 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    df <- df %>% filter(df[[col]] >= lower_bound & df[[col]] <= upper_bound)
  }
  return(df)
}

# Remove outliers for relevant columns
longitudinal_clean <- remove_outliers(Longitudinal, c("ERD_avg", "mfdfa_avg", "mfdfa_rest_avg", "mfdfa_dif_avg"))


# Summarize data for each subject over trials
longitudinal_summary <- longitudinal_clean %>%
  group_by(Subject, Trials) %>%
  summarise(ERD_avg = mean(ERD_avg),
            mfdfa_avg = mean(mfdfa_avg),
            mfdfa_rest_avg = mean(mfdfa_rest_avg),
            mfdfa_dif_avg = mean(mfdfa_dif_avg)) %>%
  ungroup()

# Plot the changes over trials for each metric
ggplot(longitudinal_summary, aes(x = Trials)) +
  geom_line(aes(y = ERD_avg, color = "ERD_avg")) +
  geom_line(aes(y = mfdfa_avg, color = "mfdfa_avg")) +
  geom_line(aes(y = mfdfa_rest_avg, color = "mfdfa_rest_avg")) +
  geom_line(aes(y = mfdfa_dif_avg, color = "mfdfa_dif_avg")) +
  labs(title = "Changes in Metrics Over Trials",
       x = "Trial",
       y = "Value",
       color = "Metric") +
  theme_minimal()


```
```{r}
performance_clean <- remove_outliers(performance_mfdfa, c("mfdfavalues", "mfdfa_dif_values", "ERDvalues", "LI_mfdfa", "LI"))

# Summarize data for each session
session_summary <- performance_clean %>%
  group_by(Session) %>%
  summarise(mfdfavalues = mean(mfdfavalues),
            mfdfa_dif_values = mean(mfdfa_dif_values),
            ERDvalues = mean(ERDvalues),
            LI = mean(LI),
            LI_MFDFA = mean(LI_mfdfa),
            accuracy = mean(accuracy)) %>%
  ungroup()


# Plot the changes over sessions for each metric
ggplot(session_summary, aes(x = Session)) +
  geom_line(aes(y = mfdfavalues, color = "mfdfavalues")) +
  geom_line(aes(y = mfdfa_dif_values, color = "mfdfa_dif_values")) +
  geom_line(aes(y = ERDvalues, color = "ERDvalues")) +
  geom_line(aes(y = LI, color = "LI")) +
  geom_line(aes(y = LI_MFDFA, color = "LI_MFDFA")) +
  geom_line(aes(y = accuracy, color = "accuracy")) +
  labs(title = "Changes in Metrics Over Sessions",
       x = "Session",
       y = "Value",
       color = "Metric") +
  theme_minimal()



kruskal_LI <- kruskal.test(LI ~ Session, data = performance_clean)
print(kruskal_LI)

# Perform Kruskal-Wallis test for LI_mfdfa
kruskal_LI_mfdfa <- kruskal.test(LI_mfdfa ~ Session, data = performance_clean)
print(kruskal_LI_mfdfa)

# If significant differences are found, perform Dunn's test for pairwise comparisons
if(kruskal_LI$p.value < 0.05) {
  dunn_LI <- dunnTest(LI ~ Session, data = performance_clean, method="bonferroni")
  print(dunn_LI)
}

if(kruskal_LI_mfdfa$p.value < 0.05) {
  dunn_LI_mfdfa <- dunnTest(LI_mfdfa ~ Session, data = performance_clean, method="bonferroni")
  print(dunn_LI_mfdfa)
}

```


```{r}
library(lme4)
library(lmerTest)
library(effectsize)
library(dplyr)
library(easystats)
# Function to calculate and summarize results
# Load necessary libraries


# Function to calculate and summarize results with non-standardized and standardized coefficients, and p-values
summarize_model_with_p <- function(model) {
  params <- model_parameters(model, standardize = "basic")
  summary_params <- summary(model)$coefficients
  data.frame(
    Effect = rownames(summary_params),
    Estimate = summary_params[, "Estimate"],
    Std_Error = summary_params[, "Std. Error"],
    p_value = summary_params[, "Pr(>|t|)"],
    Standardized_Coefficient = params$Std_Coefficient
  )
}

# Fit models and summarize results with p-values
model_erd <- lmer(ERD_avg ~ Trials + (1 | Subject), data = longitudinal_clean)
summary_erd <- summarize_model_with_p(model_erd)

model_mfdfa <- lmer(mfdfa_avg ~ Trials + (1 | Subject), data = longitudinal_clean)
summary_mfdfa <- summarize_model_with_p(model_mfdfa)

model_mfdfa_rest <- lmer(mfdfa_rest_avg ~ Trials + (1 | Subject), data = longitudinal_clean)
summary_mfdfa_rest <- summarize_model_with_p(model_mfdfa_rest)

model_mfdfa_dif <- lmer(mfdfa_dif_avg ~ Trials + (1 | Subject), data = longitudinal_clean)
summary_mfdfa_dif <- summarize_model_with_p(model_mfdfa_dif)

# Combine results into one table
results_trials <- bind_rows(
  summary_erd %>% mutate(Metric = "ERD_avg"),
  summary_mfdfa %>% mutate(Metric = "mfdfa_avg"),
  summary_mfdfa_rest %>% mutate(Metric = "mfdfa_rest_avg"),
  summary_mfdfa_dif %>% mutate(Metric = "mfdfa_dif_avg")
) %>%
  filter(Effect == "Trials")

# Print the updated results table
print(results_trials)





```


```{r}
# Fit models and summarize results
model_mfdfavalues <- lmer(mfdfa_avg ~ Session + (1 | Subject), data = longitudinal_clean)
summary_mfdfavalues <- summarize_anova(model_mfdfavalues)

model_mfdfa_dif_values <- lmer(mfdfa_dif_avg ~ Session + (1 | Subject), data = longitudinal_clean)
summary_mfdfa_dif_values <- summarize_anova(model_mfdfa_dif_values)

model_ERDvalues <- lmer(ERD_avg ~ Session + (1 | Subject), data = longitudinal_clean)
summary_ERDvalues <- summarize_anova(model_ERDvalues)

model_LI <- lmer(LI ~ Session + (1 | Subject), data = performance_clean)
summary_LI <- summarize_anova(model_LI)

model_LI_MFDFA <- lmer(LI_mfdfa ~ Session + (1 | Subject), data = performance_clean)
summary_LI_MFDFA <- summarize_anova(model_LI_MFDFA)

model_accuracy <- lmer(accuracy ~ Session + (1 | Subject), data = performance_clean)
summary_accuracy <- summarize_anova(model_accuracy)


summary_mfdfavalues <- summarize_model_with_p(model_mfdfavalues)
summary_mfdfa_dif_values <- summarize_model_with_p(model_mfdfa_dif_values)
summary_ERDvalues <- summarize_model_with_p(model_ERDvalues)
summary_LI <- summarize_model_with_p(model_LI)
summary_LI_MFDFA <- summarize_model_with_p(model_LI_MFDFA)
summary_accuracy <- summarize_model_with_p(model_accuracy)

# Combine results into one table
results_sessions <- bind_rows(
  summary_mfdfavalues %>% mutate(Metric = "mfdfavalues"),
  summary_mfdfa_dif_values %>% mutate(Metric = "mfdfa_dif_values"),
  summary_ERDvalues %>% mutate(Metric = "ERDvalues"),
  summary_LI %>% mutate(Metric = "LI"),
  summary_LI_MFDFA %>% mutate(Metric = "LI_MFDFA"),
  summary_accuracy %>% mutate(Metric = "accuracy")
) %>%
  filter(Effect == "Session")

print(results_sessions)
```